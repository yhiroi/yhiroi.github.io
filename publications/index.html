<!DOCTYPE html> <html lang="en-us"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Yuichi Hiroi </title> <meta name="author" content="Yuichi Hiroi"> <meta name="description" content="Personal research website of Yuichi Hiroi, Ph. D., investigating augmented reality, near-eye dispay optics and vision augmentation"> <meta name="keywords" content="augmented reality, optical see-through head-mounted displays, realistic visual appearance reproduction, vision measurement and vision augmentation"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/prof_hiroi_sq.jpg?28e415bc26f56eafe3304914f867cfb1"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yhiroi.github.io/publications/"> <script src="/assets/js/theme.js?7ff7ff8a7e8fb7efb59dde1e7b145b6f"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yuichi</span> Hiroi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/biography/">biography </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="nav-item active"> <a class="nav-link" href="/ja/publications/"> JA</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description"></p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#C32B72"> <div>IEEE TVCG</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2025-chromagazer-480.webp 480w,/assets/img/publication_preview/2025-chromagazer-800.webp 800w,/assets/img/publication_preview/2025-chromagazer-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2025-chromagazer.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2025-chromagazer.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tosa2025chromagazer" class="col-sm-8"> <div class="title">ChromaGazer: Unobtrusive Visual Modulation using Imperceptible Color Vibration for Visual Guidance</div> <div class="author"> Rinto Tosa, <a href="https://llhi.org/" rel="external nofollow noopener" target="_blank">Shingo Hattori</a>, <em>Yuichi Hiroi</em>, <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a>, and <a href="https://mvmlab.jp/jp/" rel="external nofollow noopener" target="_blank">Takefumi Hiraki</a> </div> <div class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2412.17274" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2025_IEEEVR_ChromaGazer.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Visual guidance (VG) plays an essential role in directing user attention in virtual reality (VR) and augmented reality (AR) environments. However, traditional approaches rely on explicit visual annotations, which often compromise visual clarity and increase user cognitive load. To address this issue, we propose an unobtrusive VG technique based on color vibration, a phenomenon in which rapidly alternating colors at frequencies above 25 Hz are perceived as a single intermediate color. Our work explores a perceptual state that exists between complete color fusion and visible flicker, where color differences remain detectable without conscious awareness of vibration. Through two experimental studies, we first identified the thresholds separating complete fusion, this intermediate perceptual state, and visible flicker by systematically varying color vibration parameters. Subsequently, we applied color vibrations with derived thresholds to natural image regions and validated their attention-guiding capabilities using eye-tracking measurements. The results demonstrate that controlled color vibration successfully directs user attention while maintaining low cognitive demand, providing an effective method for implementing unobtrusive VG in VR and AR systems.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tosa2025chromagazer</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tosa, Rinto and Hattori, Shingo and Hiroi, Yuichi and Itoh, Yuta and Hiraki, Takefumi}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Visualization and Computer Graphics}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ChromaGazer: Unobtrusive Visual Modulation using Imperceptible Color Vibration for Visual Guidance}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{to appear}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Visual Guidance;Imperceptible Color Vibration;Color Perception;Augmented Reality}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#C32B72"> <div>IEEE Access</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2025-magicitem-480.webp 480w,/assets/img/publication_preview/2025-magicitem-800.webp 800w,/assets/img/publication_preview/2025-magicitem-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2025-magicitem.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2025-magicitem.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kurai2025magicitem" class="col-sm-8"> <div class="title">MagicItem: Dynamic Behavior Design of Virtual Objects With Large Language Models in a Commercial Metaverse Platform</div> <div class="author"> Ryutaro Kurai, <a href="https://mvmlab.jp/jp/" rel="external nofollow noopener" target="_blank">Takefumi Hiraki</a>, <em>Yuichi Hiroi</em>, <a href="https://yutarohirao.notion.site/Yutaro-Hirao-s-CV-08a37b7f88f44d66baa0cb71d6838478" rel="external nofollow noopener" target="_blank">Yutaro Hirao</a>, <a href="https://www.monicaperusquia.com/index.html" rel="external nofollow noopener" target="_blank">Monica Perusquia-Hernandez</a>, Hideki Uchiyama, and <a href="https://carelab.info/ja/kiyoshi-kiyokawa/" rel="external nofollow noopener" target="_blank">Kiyoshi Kiyokawa</a> </div> <div class="periodical"> <em>IEEE Access</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ACCESS.2025.3530439" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2406.13242" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p> To create rich experiences in virtual reality (VR) environments, it is essential to define the behavior of virtual objects through programming. However, programming in 3D spaces requires a wide range of background knowledge and programming skills. Although Large Language Models (LLMs) have provided programming support, they are still primarily aimed at programmers. In metaverse platforms, where many users inhabit VR spaces, most users are unfamiliar with programming, making it difficult for them to modify the behavior of objects in the VR environment easily. Existing LLM-based script generation methods for VR spaces require multiple lengthy iterations to implement the desired behaviors and are difficult to integrate into the operation of metaverse platforms. To address this issue, we propose a tool that generates behaviors for objects in VR spaces from natural language within Cluster, a metaverse platform with a large user base. By integrating LLMs with the Cluster Script provided by this platform, we enable users with limited programming experience to define object behaviors within the platform freely. We have also integrated our tool into a commercial metaverse platform and are conducting online experiments with 63 general users of the platform. The experiments show that even users with no programming background can successfully generate behaviors for objects in VR spaces, resulting in a highly satisfying system. Our research contributes to democratizing VR content creation by enabling non-programmers to design dynamic behaviors for virtual objects in metaverse platforms.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kurai2025magicitem</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kurai, Ryutaro and Hiraki, Takefumi and Hiroi, Yuichi and Hirao, Yutaro and Perusquia-Hernandez, Monica and Uchiyama, Hideki and Kiyokawa, Kiyoshi}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Access}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MagicItem: Dynamic Behavior Design of Virtual Objects With Large Language Models in a Commercial Metaverse Platform}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{13}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{19132-19143}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ACCESS.2025.3530439}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Metaverse;Codes;Three-dimensional displays;Large language models;Natural languages;Aerospace electronics;Web servers;Usability;Programming profession;Hands;Large-language model;low-code programming;metaverse platform;virtual reality}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Augmented Humans</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2025-chromagazer-hmd-480.webp 480w,/assets/img/publication_preview/2025-chromagazer-hmd-800.webp 800w,/assets/img/publication_preview/2025-chromagazer-hmd-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2025-chromagazer-hmd.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2025-chromagazer-hmd.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="shen2025chromagazerhmd" class="col-sm-8"> <div class="title">Slim Diffractive Waveguide Glasses for Beaming Displays with Enhanced Head Orientation Tolerance</div> <div class="author"> Youfang Shen, Rinto Tosa, <a href="https://yunolv3.work/" rel="external nofollow noopener" target="_blank">Yuji Hatada</a>, <em>Yuichi Hiroi</em>, <a href="https://mvmlab.jp/jp/" rel="external nofollow noopener" target="_blank">Takefumi Hiraki</a>, and <a href="http://nae-lab.org/" rel="external nofollow noopener" target="_blank">Takeshi Naemura</a> </div> <div class="periodical"> <em>In Proceedings of the Augmented Humans International Conference 2023</em>, Mar 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2025_AHs_ChromaGazerHMD.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>VR content often involves vast spaces for users to explore, making it uncertain whether they will notice areas or objects highlighted by content creators. While various methods for guiding users’ gaze have been proposed, those that significantly alter the appearance of the content can reduce its naturalness and negatively impact the user experience. In this study, we explored a Visual Guidance (VG) approach using color vibration, based on the human visual system’s inability to perceive rapid chromatic changes. We evaluated its applicability to HMD environments and introduced a method called complementary color vibration, where opposite-phase vibrations are applied to the left and right displays of the HMD.We further investigated optimal parameters for VG and assessed its effectiveness in search tasks. Our experiments revealed that color vibration in HMD environments significantly reduced search times and areas compared to conditions without guidance, while maintaining the naturalness of the content. Furthermore, complementary color vibration was found to preserve naturalness significantly better than conventional synchronous color vibration. These findings indicate that VG using color vibration is effective in HMD environments and that complementary color vibration is less obtrusive than traditional synchronous color vibration.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">shen2025chromagazerhmd</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Shen, Youfang and Tosa, Rinto and Hatada, Yuji and Hiroi, Yuichi and Hiraki, Takefumi and Naemura, Takeshi}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Slim Diffractive Waveguide Glasses for Beaming Displays with Enhanced Head Orientation Tolerance}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Augmented Humans International Conference 2023}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{to appear}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{gaze guidance; color vibration; perceptual response; VR; head-mounted display}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE VR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2025-hoebeaming-480.webp 480w,/assets/img/publication_preview/2025-hoebeaming-800.webp 800w,/assets/img/publication_preview/2025-hoebeaming-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2025-hoebeaming.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2025-hoebeaming.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="itoh2025hoebeaming" class="col-sm-8"> <div class="title">Slim Diffractive Waveguide Glasses for Beaming Displays with Enhanced Head Orientation Tolerance</div> <div class="author"> <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a>, <a href="https://sites.google.com/site/tnakamura1104/" rel="external nofollow noopener" target="_blank">Tomoya Nakamura</a>, <em>Yuichi Hiroi</em>, and <a href="https://kaanaksit.com/" rel="external nofollow noopener" target="_blank">Kaan Aksit</a> </div> <div class="periodical"> <em>In 2025 IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)</em>, Mar 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2025_IEEEVR_HOE_Beaming.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Augmented Reality (AR) glasses must be slim, lightweight, and energy-efficient to achieve widespread adoption. Beaming Displays present a promising solution by offloading active components, such as the power-supplied light engine, into the surrounding environment while leaving only passive elements, like the eyepiece, in the wearable device. However, existing approaches still struggle to achieve both a slim design and a wide tolerance for projection angles relative to the user’s head orientation. In this work, we introduce a design for light-receiving glasses using a diffractive waveguide with in-coupling and out-coupling gratings. Our approach expands the allowable range of incident angles while maintaining a compact, lightweight form factor. We developed a proof-of-concept prototype and demonstrated an incident angle tolerance of approximately 20-30 degrees range, overcoming the previous design of 5 degrees.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">itoh2025hoebeaming</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Itoh, Yuta and Nakamura, Tomoya and Hiroi, Yuichi and Aksit, Kaan}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Slim Diffractive Waveguide Glasses for Beaming Displays with Enhanced Head Orientation Tolerance}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2025 IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{to appear}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{beaming display; augmented reality; waveguide; near-eye displays; DOEs}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#666666"> <div>IEEE VR Poster</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2025-magic-craft-poster-480.webp 480w,/assets/img/publication_preview/2025-magic-craft-poster-800.webp 800w,/assets/img/publication_preview/2025-magic-craft-poster-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2025-magic-craft-poster.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2025-magic-craft-poster.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kurai2025magiccraft-poster" class="col-sm-8"> <div class="title">An implementation of MagicCraft: Generating Interactive 3D Objects and Their Behaviors from Text for Commercial Metaverse Platforms</div> <div class="author"> Ryutaro Kurai, <a href="https://mvmlab.jp/jp/" rel="external nofollow noopener" target="_blank">Takefumi Hiraki</a>, <em>Yuichi Hiroi</em>, <a href="https://yutarohirao.notion.site/Yutaro-Hirao-s-CV-08a37b7f88f44d66baa0cb71d6838478" rel="external nofollow noopener" target="_blank">Yutaro Hirao</a>, <a href="https://www.monicaperusquia.com/index.html" rel="external nofollow noopener" target="_blank">Monica Perusquia-Hernandez</a>, Hideki Uchiyama, and <a href="https://carelab.info/ja/kiyoshi-kiyokawa/" rel="external nofollow noopener" target="_blank">Kiyoshi Kiyokawa</a> </div> <div class="periodical"> <em>In 2025 IEEE Conference on Virtual Reality and 3D User Interfaces Adjunct 2025</em>, Mar 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2025_IEEEVR_MagicCraft_Poster.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Metaverse platforms are rapidly evolving to provide immersive spaces. However, the generation of dynamic and interactive 3D objects remains a challenge due to the need for advanced 3D modeling and programming skills. We present MagicCraft, a system that generates functional 3D objects from natural language prompts. MagicCraft uses generative AI models to manage the entire content creation pipeline: converting user text descriptions into images, transforming images into 3D models, predicting object behavior, and assigning necessary attributes and scripts. It also provides an interactive interface for users to refine generated objects by adjusting features like orientation, scale, seating positions, and grip points.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">kurai2025magiccraft-poster</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kurai, Ryutaro and Hiraki, Takefumi and Hiroi, Yuichi and Hirao, Yutaro and Perusquia-Hernandez, Monica and Uchiyama, Hideki and Kiyokawa, Kiyoshi}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{An implementation of MagicCraft: Generating Interactive 3D Objects and Their Behaviors from Text for Commercial Metaverse Platforms}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2025 IEEE Conference on Virtual Reality and 3D User Interfaces Adjunct 2025}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{to appear}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Metaverse;3D Object Generation;Generative AI;AI-Assisted Design;Spatio-temporal Light pattern}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#666666"> <div>IEEE VR Workshop</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2025-beamstellar-480.webp 480w,/assets/img/publication_preview/2025-beamstellar-800.webp 800w,/assets/img/publication_preview/2025-beamstellar-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2025-beamstellar.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2025-beamstellar.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="weigand2025stellar" class="col-sm-8"> <div class="title">BeamStellar: Low-Latency, 6-DoF Head Tracking for Beaming Displays with Spatio-Temporal LED Encoding</div> <div class="author"> Jonas Weigand, <em>Yuichi Hiroi</em>, and <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a> </div> <div class="periodical"> <em>In 2025 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)</em>, Mar 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2025_IEEEVR_WS_BeamStellar.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Conventional augmented reality (AR) head-mounted displays (HMDs) require the integration of all components, including optics and processors, which presents significant barriers to practical implementation due to weight and heat generation concerns. While Beaming Displays address these challenges by offloading computing and projection functions to the environment, existing solutions leaving 6-degrees of freedom (6DoF) image projection with low motion-to-photon (M2P) latency. In this position paper, we introduce our trial BeamStellar, a Beaming Display system that integrates 6 DoF head tracking by combining spatio-temporally encoded LED patterns with a position-sensing detector. Through digital signal processing using an FPGA, the system aims a theoretical latency of less than 200 microseconds. This implementation alleviates the hardware burden on users, facilitating seamless AR experiences that seamlessly blend virtual content with the physical world.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">weigand2025stellar</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Weigand, Jonas and Hiroi, Yuichi and Itoh, Yuta}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{BeamStellar: Low-Latency, 6-DoF Head Tracking for Beaming Displays with Spatio-Temporal LED Encoding}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2025 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{to appear}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Beaming Displays;Optical See-through Display;Low latency head tracking;Spatio-temporal Light pattern}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#666666"> <div>IEEE VR Workshop</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2025-kajima-digitaltwin-480.webp 480w,/assets/img/publication_preview/2025-kajima-digitaltwin-800.webp 800w,/assets/img/publication_preview/2025-kajima-digitaltwin-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2025-kajima-digitaltwin.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2025-kajima-digitaltwin.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="masubuchi2025digitaltwin" class="col-sm-8"> <div class="title">Development of Digital Twin Environment through Integration of Commercial Metaverse Platform and IoT Sensors of Smart Building</div> <div class="author"> Yusuke Masubuchi, <a href="https://mvmlab.jp/jp/" rel="external nofollow noopener" target="_blank">Takefumi Hiraki</a>, <em>Yuichi Hiroi</em>, Masanori Ibara, Kazuki Matsutani, Megumi Zaizen, and Junya Morita </div> <div class="periodical"> <em>In 2025 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)</em>, Mar 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>The digital transformation of smart cities and workplaces requires effective integration of physical and cyber spaces, yet existing digital twin solutions remain limited in supporting real-time, multi-user collaboration. While metaverse platforms enable shared virtual experiences, they have not supported comprehensive integration of IoT sensors on physical spaces, especially for large-scale smart architectural environments. This paper presents a digital twin environment that integrates Kajima Corp.’s smart building facility "The GEAR" in Singapore with a commercial metaverse platform Cluster. Our system consists of three key components: a standardized IoT sensor platform, a real-time data relay system, and an environmental data visualization framework. Quantitative end-to-end latency measurements confirm the feasibility of our approach for real-world applications in large architectural spaces. The proposed framework enables new forms of collaboration that transcend spatial constraints, advancing the development of next-generation interactive environments.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">masubuchi2025digitaltwin</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Masubuchi, Yusuke and Hiraki, Takefumi and Hiroi, Yuichi and Ibara, Masanori and Matsutani, Kazuki and Zaizen, Megumi and Morita, Junya}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Development of Digital Twin Environment through Integration of Commercial Metaverse Platform and IoT Sensors of Smart Building}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2025 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{to appear}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Digital Twin;Metaverse Integration;Smart Building;IoT Sensors}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#C32B72"> <div>IEEE TVCG</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2024-haptofloater-480.webp 480w,/assets/img/publication_preview/2024-haptofloater-800.webp 800w,/assets/img/publication_preview/2024-haptofloater-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2024-haptofloater.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2024-haptofloater.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="nagano2024haptofloater" class="col-sm-8"> <div class="title">HaptoFloater: Visuo-Haptic Augmented Reality by Embedding Imperceptible Color Vibration Signals for Tactile Display Control in a Mid-Air Image</div> <div class="author"> Rina Nagano, Takahiro Kinoshita, <a href="https://llhi.org/" rel="external nofollow noopener" target="_blank">Shingo Hattori</a>, <em>Yuichi Hiroi</em>, <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a>, and <a href="https://mvmlab.jp/jp/" rel="external nofollow noopener" target="_blank">Takefumi Hiraki</a> </div> <div class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, Nov 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/TVCG.2024.3456175" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2024_ISMAR_HaptoFloater.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=Rn1AN1S1Ous" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>We propose HaptoFloater, a low-latency mid-air visuo-haptic augmented reality (VHAR) system that utilizes imperceptible color vibrations. When adding tactile stimuli to the visual information of a mid-air image, the user should not perceive the latency between the tactile and visual information. However, conventional tactile presentation methods for mid-air images, based on camera-detected fingertip positioning, introduce latency due to image processing and communication. To mitigate this latency, we use a color vibration technique; humans cannot perceive the vibration when the display alternates between two different color stimuli at a frequency of 25 Hz or higher. In our system, we embed this imperceptible color vibration into the mid-air image formed by a micromirror array plate, and a photodiode on the fingertip device directly detects this color vibration to provide tactile stimulation. Thus, our system allows for the tactile perception of multiple patterns on a mid-air image in 59.5 ms. In addition, we evaluate the visual-haptic delay tolerance on a mid-air display using our VHAR system and a tactile actuator with a single pattern and faster response time. The results of our user study indicate a visual-haptic delay tolerance of 110.6 ms, which is considerably larger than the latency associated with systems using multiple tactile patterns.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">nagano2024haptofloater</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nagano, Rina and Kinoshita, Takahiro and Hattori, Shingo and Hiroi, Yuichi and Itoh, Yuta and Hiraki, Takefumi}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Visualization and Computer Graphics}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{HaptoFloater: Visuo-Haptic Augmented Reality by Embedding Imperceptible Color Vibration Signals for Tactile Display Control in a Mid-Air Image}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{30}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{7463-7472}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Haptic interfaces;Vibrations;Visualization;Image color analysis;Actuators;Delays;Tactile sensors;Visuo-haptic displays;mid-air images;LCD displays;imperceptible color vibration}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TVCG.2024.3456175}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1941-0506}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#C32B72"> <div>IEEE TVCG</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2024-stained-sweeper-480.webp 480w,/assets/img/publication_preview/2024-stained-sweeper-800.webp 800w,/assets/img/publication_preview/2024-stained-sweeper-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2024-stained-sweeper.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2024-stained-sweeper.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hiroi2024stainedsweeper" class="col-sm-8"> <div class="title">StainedSweeper: Compact, Variable-Intensity Light-Attenuation Display with Sweeping Tunable Retarders</div> <div class="author"> <em>Yuichi Hiroi</em>, <a href="https://mvmlab.jp/jp/" rel="external nofollow noopener" target="_blank">Takefumi Hiraki</a>, and <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a> </div> <div class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/TVCG.2024.3372058" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2024_IEEEVR_StainedSweeper.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/2024_IEEEVR_StainedSweeper_Supplementary.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://www.youtube.com/watch?v=ASugkZMet3o" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Light Attenuation Displays (LADs) are a type of Optical See-Through Head-Mounted Display (OST-HMD) that present images by attenuating incoming light with a pixel-wise polarizing color filter. Although LADs can display images in bright environments, there is a trade-off between the number of Spatial Light Modulators (SLMs) and the color gamut and contrast that can be expressed, making it difficult to achieve both high-fidelity image display and a small form factor. To address this problem, we propose StainedSweeper, a LAD that achieves both the wide color gamut and the variable intensity with a single SLM. Our system synchronously controls a pixel-wise Digital Micromirror Device (DMD) and a nonpixel polarizing color filter to pass light when each pixel is the desired color. By sweeping this control at high speed, the human eye perceives images in a time-multiplexed, integrated manner. To achieve this, we develop the OST-HMD design using a reflective Solc filter as a polarized color filter and a color reproduction algorithm based on the optimization of the time-multiplexing matrix for the selected primary color filters. Our proof-of-concept prototype showed that our single SLM design can produce subtractive images with variable contrast and a wider color gamut than conventional LADs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hiroi2024stainedsweeper</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hiroi, Yuichi and Hiraki, Takefumi and Itoh, Yuta}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Visualization and Computer Graphics}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{StainedSweeper: Compact, Variable-Intensity Light-Attenuation Display with Sweeping Tunable Retarders}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{30}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2682-2692}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Image color analysis;Optical filters;Filtering algorithms;Light sources;Optical imaging;Optical reflection;Optical polarization;Light attenuation display;see-through display;augmented reality;time-multiplexing;polarized color filter}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TVCG.2024.3372058}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1941-0506}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#C32B72"> <div>IEEE TVCG</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2024-dual-beaming-480.webp 480w,/assets/img/publication_preview/2024-dual-beaming-800.webp 800w,/assets/img/publication_preview/2024-dual-beaming-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2024-dual-beaming.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2024-dual-beaming.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="aoki2024dualbeaming" class="col-sm-8"> <div class="title">Towards Co-Operative Beaming Displays: Dual Steering Projectors for Extended Projection Volume and Head Orientation Range</div> <div class="author"> Hiroto Aoki, Takumi Tochimoto, <em>Yuichi Hiroi</em>, and <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a> </div> <div class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Honorable Mention</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/TVCG.2024.3372118" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2024_IEEEVR_DualBeaming.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="award hidden d-print-inline"> <p> </p> </div> <div class="abstract hidden"> <p>Existing near-eye displays (NEDs) have trade-offs related to size, weight, computational resources, battery life, and body temperature. A recent paradigm, beaming display, addresses these trade-offs by separating the NED into a steering projector (SP) for image presentation and a passive headset worn by the user. However, the beaming display has issues with the projection area of a single SP and has severe limitations on the head orientation and pose that the user can move. In this study, we distribute dual steering projectors in the scene to extend the head orientation and pose of the beaming display by coordinating the dual projections on a passive headset. For cooperative control of each SP, we define a geometric model of the SPs and propose a calibration and projection control method designed for multiple projectors. We present implementations of the system along with evaluations showing that the precision and delay are 1.8 ∼ 5.7 mm and 14.46 ms, respectively, at a distance of about 1 m from the SPs. From this result, our prototype with multiple SPs can project images in the projection area (20 \textmm \times 30 \textmm) of the passive headset while extending the projectable head orientation. Furthermore, as applications of cooperative control by multiple SPs, we show the possibility of multiple users, improving dynamic range and binocular presentation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">aoki2024dualbeaming</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Aoki, Hiroto and Tochimoto, Takumi and Hiroi, Yuichi and Itoh, Yuta}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Visualization and Computer Graphics}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards Co-Operative Beaming Displays: Dual Steering Projectors for Extended Projection Volume and Head Orientation Range}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{30}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2309-2318}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Mirrors;Cameras;Headphones;Calibration;Optical imaging;High-speed optical techniques;Head;Near-eye display;Augmented reality;Projectors}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TVCG.2024.3372118}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1941-0506}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE ISMAR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2024-factored-sweeper-480.webp 480w,/assets/img/publication_preview/2024-factored-sweeper-800.webp 800w,/assets/img/publication_preview/2024-factored-sweeper-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2024-factored-sweeper.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2024-factored-sweeper.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hiroi2024factored" class="col-sm-8"> <div class="title">FactoredSweeper: Optical See-Through Display Integrating Light Attenuation and Addition with a Single Spatial Light Modulator</div> <div class="author"> <em>Yuichi Hiroi</em>, <a href="https://mvmlab.jp/jp/" rel="external nofollow noopener" target="_blank">Takefumi Hiraki</a>, and <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a> </div> <div class="periodical"> <em>In 2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/2024_ISMAR_FactoredSweeper.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/2024_ISMAR_FactoredSweeper_supplement.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://www.youtube.com/watch?v=z2S89fqwF5E" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Light Attenuation Displays (LADs), a subset of Optical See-Through Head-Mounted Displays (OST-HMDs), enable image display in bright environments by filtering incident light at the pixel level. Although recent methods have proposed single-DMD light attenuation, they do not consider additive color display and background compensation, limiting their applicability in real-world scenarios. We present FactoredSweeper, a single digital micromirror device (DMD) system that incorporates both light attenuation and addition. By synchronizing the DMD, color filter, and light source, our system generates an additive virtual image, light attenuation, and occlusion through time multiplexing. To produce the target image while compensating for the background, we optimize time-multiplexed binary DMD patterns and LED/color filter schedules using perceptually-driven non-negative matrix factorization. Simulations and prototypes demonstrate that our integrated attenuation-addition single-SLM system achieves superior dynamic range and perceptual image quality compared to conventional occlusion-capable OST-HMDs using grayscale occlusion masks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#666666"> <div>ISMAR Demo</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2024-metagadget-480.webp 480w,/assets/img/publication_preview/2024-metagadget-800.webp 800w,/assets/img/publication_preview/2024-metagadget-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2024-metagadget.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2024-metagadget.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kurai2024metagadget" class="col-sm-8"> <div class="title"> MetaGadget: IoT Framework for Event-Triggered Integration of User-Developed Devices into Commercial Metaverse Platforms </div> <div class="author"> Ryutaro Kurai, <em>Yuichi Hiroi</em>, and <a href="https://mvmlab.jp/jp/" rel="external nofollow noopener" target="_blank">Takefumi Hiraki</a> </div> <div class="periodical"> <em>In 2024 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct) </em>, Nov 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.youtube.com/watch?v=yg-MRhy0TvM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/cluster-lab/MetaGadget" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p> This demonstration introduces MetaGadget, an IoT framework designed to integrate user-developed devices into commercial metaverse platforms. Synchronizing virtual reality (VR) environments with physical devices has traditionally required a constant connection to VR clients, limiting flexibility and resource efficiency. MetaGadget overcomes these limitations by configuring user-developed devices as IoT units with server capabilities, supporting communication via HTTP protocols within the commercial metaverse platform, Cluster. This approach enables event-triggered device control without the need for persistent connections from metaverse clients. Through the demonstration, users will experience event-triggered interaction between VR and physical devices, as well as real-world device control through the VR space by multiple people. Our framework is expected to reduce technical barriers to integrating VR spaces and custom devices, contribute to interoperability, and increase resource efficiency through event-triggered connections. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#666666"> <div>ISMAR Poster</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2024-thin-holographic-beaming-480.webp 480w,/assets/img/publication_preview/2024-thin-holographic-beaming-800.webp 800w,/assets/img/publication_preview/2024-thin-holographic-beaming-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2024-thin-holographic-beaming.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2024-thin-holographic-beaming.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="itoh2024thinholographic" class="col-sm-8"> <div class="title">Beaming Display Using Thin Holographic Waveguides for Wider Head Orientation Angle Range</div> <div class="author"> <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a>, <a href="https://sites.google.com/site/tnakamura1104/" rel="external nofollow noopener" target="_blank">Tomoya Nakamura</a>, <em>Yuichi Hiroi</em>, and <a href="https://kaanaksit.com/" rel="external nofollow noopener" target="_blank">Kaan Aksit</a> </div> <div class="periodical"> <em>In 2024 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ISMAR-Adjunct64951.2024.00138" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2024_ISMAR_HOE_Beaming_Poster.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Augmented Reality (AR) glasses face fundamental challenges related to technical trade-offs. Emerging Beaming Displays (BDs) offer a compelling solution by separating the active and passive components. However, existing BD-based AR glasses have yet to achieve a thin and lightweight design with wide incident projection angles. This work proposes an eyepiece for BDs, including a holographic waveguide with tailored in- and out-coupling gratings. The proposed design aims to achieve a millimeter-thin form factor with a wide tolerance for incident angles, thus overcoming the limitations of existing designs. We have constructed proof-of-concept passive AR glasses prototypes, all approximately 2mm thick, including one in the form of conventional eyeglasses, and demonstrated an acceptable lateral angle of incidence of up to 90 degrees.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">itoh2024thinholographic</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Itoh, Yuta and Nakamura, Tomoya and Hiroi, Yuichi and Aksit, Kaan}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Beaming Display Using Thin Holographic Waveguides for Wider Head Orientation Angle Range}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{475-476}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Beaming display; Augmented reality; Near-eye display; waveguide; HOEs}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ISMAR-Adjunct64951.2024.00138}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#666666"> <div>arXiv</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2024-panotree-480.webp 480w,/assets/img/publication_preview/2024-panotree-800.webp 800w,/assets/img/publication_preview/2024-panotree-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2024-panotree.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2024-panotree.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hayase2024panotree" class="col-sm-8"> <div class="title">PanoTree: Autonomous Photo-Spot Explorer in Virtual Reality Scenes</div> <div class="author"> <a href="https://thayafluss.net/a5e3683ec8fb4109b3966afb02d6a4c2" rel="external nofollow noopener" target="_blank">Tomohiro Hayase</a>, Sacha Braun, Hikari Yanagawa, Itsuki Orito, and <em>Yuichi Hiroi</em> </div> <div class="periodical"> Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2405.17136" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.youtube.com/watch?v=AvDRMJX5QGs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/cluster-lab/panotree" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://cluster-lab.github.io/panotree/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">hayase2024panotree</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PanoTree: Autonomous Photo-Spot Explorer in Virtual Reality Scenes}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hayase, Tomohiro and Braun, Sacha and Yanagawa, Hikari and Orito, Itsuki and Hiroi, Yuichi}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2405.17136}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#666666"> <div>SIGGRAPH Poster</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2024-macadam-search-480.webp 480w,/assets/img/publication_preview/2024-macadam-search-800.webp 800w,/assets/img/publication_preview/2024-macadam-search-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2024-macadam-search.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2024-macadam-search.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hattori2024macadam" class="col-sm-8"> <div class="title">Measurement of the Imperceptible Threshold for Color Vibration Pairs Selected by using MacAdam Ellipse</div> <div class="author"> <a href="https://llhi.org/" rel="external nofollow noopener" target="_blank">Shingo Hattori</a>, <em>Yuichi Hiroi</em>, and <a href="https://mvmlab.jp/jp/" rel="external nofollow noopener" target="_blank">Takefumi Hiraki</a> </div> <div class="periodical"> <em>In ACM SIGGRAPH 2024 Posters</em>, Denver, CO, USA, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awarded</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3641234.3671041" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2406.08227" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>1st Place in SIGGRAPH 2024 Student Research Competition for Undergraduate Work</p> </div> <div class="abstract hidden"> <p>We propose an efficient method for searching for color vibration pairs that are imperceptible to the human eye based on the MacAdam ellipse, an experimentally determined color-difference range that is indistinguishable to the human eye. We created color pairs by selecting eight colors within the sRGB color space specified by the ellipse, and conducted experiments to confirm the threshold of the amplitude of color vibration amplitude at which flicker becomes imperceptible to the human eye. The experimental results indicate a general guideline for acceptable amplitudes for pair selection.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hattori2024macadam</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hattori, Shingo and Hiroi, Yuichi and Hiraki, Takefumi}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Measurement of the Imperceptible Threshold for Color Vibration Pairs Selected by using MacAdam Ellipse}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400705168}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3641234.3671041}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3641234.3671041}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACM SIGGRAPH 2024 Posters}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{68}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{MacAdam ellipse, color perception, imperceptible color vibration}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Denver, CO, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{SIGGRAPH '24}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#666666"> <div>IEEE VR Workshop</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2024-agent-api-480.webp 480w,/assets/img/publication_preview/2024-agent-api-800.webp 800w,/assets/img/publication_preview/2024-agent-api-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2024-agent-api.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2024-agent-api.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kurai2024agentapi" class="col-sm-8"> <div class="title"> Design and Implementation of Agent APIs for Large-Scale Social VR Platforms </div> <div class="author"> Ryutaro Kurai, <a href="https://mvmlab.jp/jp/" rel="external nofollow noopener" target="_blank">Takefumi Hiraki</a>, <em>Yuichi Hiroi</em>, <a href="https://yutarohirao.notion.site/Yutaro-Hirao-s-CV-08a37b7f88f44d66baa0cb71d6838478" rel="external nofollow noopener" target="_blank">Yutaro Hirao</a>, <a href="https://www.monicaperusquia.com/index.html" rel="external nofollow noopener" target="_blank">Monica Perusquia-Hernandez</a>, Hideaki Uchiyama, and <a href="https://carelab.info/ja/kiyoshi-kiyokawa/" rel="external nofollow noopener" target="_blank">Kiyoshi Kiyokawa</a> </div> <div class="periodical"> <em>In 2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW) </em>, Mar 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/VRW62533.2024.00112" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p> Implementing an autonomous agent on a social VR platform where many users share space requires diverse information. In particular, it is required to recognize the distance from other users, their orientation toward each other, the avatar’s pose, and text and voice messages, and to behave accordingly. This paper proposes an API to obtain the above information on “Cluster,” a multi-device social VR platform in operation, and an agent that uses the API. We have implemented this API using a network proxy. The agent using this API can connect to ChatGPT [7] and have a conversation in real time. We measured the latency required for the conversation and confirmed that the response time was about 1 second. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">kurai2024agentapi</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kurai, Ryutaro and Hiraki, Takefumi and Hiroi, Yuichi and Hirao, Yutaro and Perusquia-Hernandez, Monica and Uchiyama, Hideaki and Kiyokawa, Kiyoshi}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ 2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW) }</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{ Design and Implementation of Agent APIs for Large-Scale Social VR Platforms }}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{584-587}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Three-dimensional displays;Text recognition;Oral communication;Virtual reality;Speech recognition;User interfaces;Software}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/VRW62533.2024.00112}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.ieeecomputersociety.org/10.1109/VRW62533.2024.00112}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE Computer Society}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Los Alamitos, CA, USA}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#C32B72"> <div>IEEE TVCG</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2023-low-latency-beaming-480.webp 480w,/assets/img/publication_preview/2023-low-latency-beaming-800.webp 800w,/assets/img/publication_preview/2023-low-latency-beaming-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2023-low-latency-beaming.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2023-low-latency-beaming.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hiroi2023lowlatency" class="col-sm-8"> <div class="title">Low-Latency Beaming Display: Implementation of Wearable, 133 μs Motion-to-Photon Latency Near-Eye Display</div> <div class="author"> <em>Yuichi Hiroi</em> , Akira Watanabe, <a href="http://yurimikawa.com/index.php" rel="external nofollow noopener" target="_blank">Yuri Mikawa</a>, and <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a> </div> <div class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, Nov 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/TVCG.2023.3320212" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2023_ISMAR_Lowlatency.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=pCUf4rUgQdg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://www.youtube.com/watch?v=veqkiiwLzeM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Slides</a> </div> <div class="abstract hidden"> <p>This paper presents a low-latency Beaming Display system with a 133 μ\mathrms motion-to-photon (M2P) latency, the delay from head motion to the corresponding image motion. The Beaming Display represents a recent near-eye display paradigm that involves a steerable remote projector and a passive wearable headset. This system aims to overcome typical trade-offs of Optical See-Through Head-Mounted Displays (OST-HMDs), such as weight and computational resources. However, since the Beaming Display projects a small image onto a moving, distant viewpoint, M2P latency significantly affects displacement. To reduce M2P latency, we propose a low-latency Beaming Display system that can be modularized without relying on expensive high-speed devices. In our system, a 2D position sensor, which is placed coaxially on the projector, detects the light from the IR-LED on the headset and generates a differential signal for tracking. An analog closed-loop control of the steering mirror based on this signal continuously projects images onto the headset. We have implemented a proof-of-concept prototype, evaluated the latency and the augmented reality experience through a user-perspective camera, and discussed the limitations and potential improvements of the prototype.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hiroi2023lowlatency</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hiroi, Yuichi and Watanabe, Akira and Mikawa, Yuri and Itoh, Yuta}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Visualization and Computer Graphics}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Low-Latency Beaming Display: Implementation of Wearable, 133 μs Motion-to-Photon Latency Near-Eye Display}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{29}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4761-4771}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Mirrors;Low latency communication;Headphones;Head;Cameras;Tracking;Two dimensional displays;Low-Latency Display;Beaming Display;Motion-to-Photon Latency;Lateral-effect Photodiodes}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TVCG.2023.3320212}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1941-0506}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACM UIST</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2023-telextiles-480.webp 480w,/assets/img/publication_preview/2023-telextiles-800.webp 800w,/assets/img/publication_preview/2023-telextiles-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2023-telextiles.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2023-telextiles.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kitagishi2023telextiles" class="col-sm-8"> <div class="title">Telextiles: End-to-end Remote Transmission of Fabric Tactile Sensation</div> <div class="author"> <a href="https://takekazukitagishi.studio.site/" rel="external nofollow noopener" target="_blank">Takekazu Kitagishi</a>, <em>Yuichi Hiroi</em>, <a href="https://ynwtnb.github.io/portfolio/" rel="external nofollow noopener" target="_blank">Yuna Watanabe</a>, <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a>, and <a href="https://lab.rekimoto.org/members/rekimoto/" rel="external nofollow noopener" target="_blank">Jun Rekimoto</a> </div> <div class="periodical"> <em>In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology</em>, San Francisco, CA, USA, Nov 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3586183.3606764" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.youtube.com/watch?v=be8iWmW6AUA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://lab.rekimoto.org/projects/telextiles/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>The tactile sensation of textiles is critical in determining the comfort of clothing. For remote use, such as online shopping, users cannot physically touch the textile of clothes, making it difficult to evaluate its tactile sensation. Tactile sensing and actuation devices are required to transmit the tactile sensation of textiles. The sensing device needs to recognize different garments, even with hand-held sensors. In addition, the existing actuation device can only present a limited number of known patterns and cannot transmit unknown tactile sensations of textiles. To address these issues, we propose Telextiles, an interface that can remotely transmit tactile sensations of textiles by creating a latent space that reflects the proximity of textiles through contrastive self-supervised learning. We confirm that textiles with similar tactile features are located close to each other in the latent space through a two-dimensional plot. We then compress the latent features for known textile samples into the 1D distance and apply the 16 textile samples to the rollers in the order of the distance. The roller is rotated to select the textile with the closest feature if an unknown textile is detected.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">kitagishi2023telextiles</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kitagishi, Takekazu and Hiroi, Yuichi and Watanabe, Yuna and Itoh, Yuta and Rekimoto, Jun}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Telextiles: End-to-end Remote Transmission of Fabric Tactile Sensation}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400701320}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3586183.3606764}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3586183.3606764}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{67}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{10}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Haptic feedback, Machine learning, Passive haptic feedback, Self supervised learning, Tactile Display, Tactile perception, Texture, Texture perception, Texture recognition}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{San Francisco, CA, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{UIST '23}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACM VRST</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2023-retinal-homing-480.webp 480w,/assets/img/publication_preview/2023-retinal-homing-800.webp 800w,/assets/img/publication_preview/2023-retinal-homing-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2023-retinal-homing.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2023-retinal-homing.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="aoki2023retinal" class="col-sm-8"> <div class="title">Retinal Homing Display: Head-Tracking Auto-stereoscopic Retinal Projection Display</div> <div class="author"> Hiroto Aoki, <em>Yuichi Hiroi</em>, <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a>, and <a href="https://lab.rekimoto.org/members/rekimoto/" rel="external nofollow noopener" target="_blank">Jun Rekimoto</a> </div> <div class="periodical"> <em>In Proceedings of the 29th ACM Symposium on Virtual Reality Software and Technology</em>, Christchurch, New Zealand, Nov 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3611659.3615715" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2023_VRST_RetinalHoming.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper introduces Retinal Homing Display, which presents focus-free stereoscopic images via retinal projection, thus eliminating the need for the user to wear additional equipment. Traditional 3D displays, typically classified as either naked-eye stereoscopic or wearable, present inherent challenges: the former involves a compromise between resolution and accurate depth perception, while the latter imposes an additional burden on the user. Our proposed display employs optical and mechanical mechanisms to converge projector light at the user’s pupil center, simultaneously tracking eye movements. This lets the user perceive focus-free, high-resolution stereoscopic images without wearable equipment. We implemented a proof-of-concept system utilizing a robotic arm and a Dihedral Corner Reflector Array (DCRA), subsequently evaluating image quality and its eyebox. Finally, we discuss the limitations of the current prototype and outline potential directions for future research.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">aoki2023retinal</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Aoki, Hiroto and Hiroi, Yuichi and Itoh, Yuta and Rekimoto, Jun}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Retinal Homing Display: Head-Tracking Auto-stereoscopic Retinal Projection Display}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400703287}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3611659.3615715}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3611659.3615715}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 29th ACM Symposium on Virtual Reality Software and Technology}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{13}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{10}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{autostereoscopic display, motion-following display, retinal projection}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Christchurch, New Zealand}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{VRST '23}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE VR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2023-photochromic-occlusion-480.webp 480w,/assets/img/publication_preview/2023-photochromic-occlusion-800.webp 800w,/assets/img/publication_preview/2023-photochromic-occlusion-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2023-photochromic-occlusion.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2023-photochromic-occlusion.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ooi2023photochromic" class="col-sm-8"> <div class="title">A Compact Photochromic Occlusion Capable See-through Display with Holographic Lenses</div> <div class="author"> <a href="https://imjyun.com/" rel="external nofollow noopener" target="_blank">Chun-Wei Ooi</a>, <em>Yuichi Hiroi</em>, and <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a> </div> <div class="periodical"> <em>In 2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)</em>, Mar 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/VR55154.2023.00039" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2023_IEEEVR_CompactPhotochro.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Occlusion is a crucial visual element in optical see-through (OST) augmented reality, however, implementing occlusion in OST displays while addressing various design trade-offs is a difficult problem. In contrast to the traditional method of using spatial light modulators (SLMs) for the occlusion mask, using photochromic materials as occlusion masks can effectively eliminate diffraction artifacts in see-through views due to the lack of electronic pixels, thus providing superior see-through image quality. However, this design requires UV illumination to activate the photochromic mate-rial, which traditionally requires multiple SLMs, resulting in a larger form factor for the system. This paper presents a compact photochromic occlusion-capable OST design using multilayer, wavelength-dependent holographic optical lenses (HOLs). Our approach employs a single digital mi-cromirror display (DMD) to form both the occlusion mask with UV light and a virtual image with visible light in a time-multiplexed man-ner. We demonstrate our proof-of-concept system on a bench-top setup and assess the appearance and contrasts of the displayed image. We also suggest potential improvements for current prototypes to encourage the community to explore this occlusion approach.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ooi2023photochromic</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ooi, Chun-Wei and Hiroi, Yuichi and Itoh, Yuta}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Compact Photochromic Occlusion Capable See-through Display with Holographic Lenses}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{237-242}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Visualization;Optical diffraction;Three-dimensional displays;Optical design;Prototypes;Holography;User interfaces;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented reality;Human-centered computing-Communication hardware;interfaces and storage-Displays and imagers}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/VR55154.2023.00039}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2642-5254}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#666666"> <div>ISMAR Poster</div> </abbr> </div> <div id="tochimoto2023dualbeaming" class="col-sm-8"> <div class="title">Dual Beaming Display for Extended Head Orientation and Projection Volume</div> <div class="author"> Takumi Tochimoto, <em>Yuichi Hiroi</em>, and <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a> </div> <div class="periodical"> <em>In 2023 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ISMAR-Adjunct60411.2023.00081" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Existing near-eye displays (NEDs) come with various tradeoffs, including size, weight, and battery life. The new "beaming display" model attempts to address these by splitting the NED into a steering projector (SP) and a passive headset. However, it has limitations in terms of projection area and user head movement. In this research, we utilize two steering projectors to expand the beaming display’s capabilities. We introduce a geometric model and a control method specifically for multiple projectors. Tests indicate an accuracy of 3-19mm and a delay of 14.46 ms from a 1m distance to the SPs, allowing projection onto a 20mm x 30mm area of the headset. This method not only expands head movement possibilities but also shows potential for multiple users and improved dynamic range and binocular presentation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tochimoto2023dualbeaming</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tochimoto, Takumi and Hiroi, Yuichi and Itoh, Yuta}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dual Beaming Display for Extended Head Orientation and Projection Volume}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{377-378}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Headphones;Design methodology;Geometric modeling;Display systems;Focusing;Dynamic range;Delays;Human-centered computing;Visualization;Visualization techniques;Treemaps; Human-centered computing;Visualization design and evaluation methods}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ISMAR-Adjunct60411.2023.00081}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2771-1110}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#666666"> <div>AHs Poster</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2023-directional-auditory-480.webp 480w,/assets/img/publication_preview/2023-directional-auditory-800.webp 800w,/assets/img/publication_preview/2023-directional-auditory-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2023-directional-auditory.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2023-directional-auditory.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="koike2023braincomputer" class="col-sm-8"> <div class="title">Brain-Computer Interface using Directional Auditory Perception</div> <div class="author"> Yuto Koike, <em>Yuichi Hiroi</em>, <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a>, and <a href="https://lab.rekimoto.org/members/rekimoto/" rel="external nofollow noopener" target="_blank">Jun Rekimoto</a> </div> <div class="periodical"> <em>In Proceedings of the Augmented Humans International Conference 2023</em>, Glasgow, United Kingdom, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awarded</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3582700.3583713" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Best Poster Honorable Mention</p> </div> <div class="abstract hidden"> <p>We investigate the potential of brain-computer interface (BCI) using electroencephalogram (EEG) induced by listening (or recalling) auditory stimuli of different directions. In the initial attempt, we apply a time series classification model based on deep learning to the EEG to demonstrate whether each EEG can be classified by recognizing binary (left or right) auditory directions. The results showed high classification accuracy when trained and tested on the same users. Discussion is provided to further explore this topic.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">koike2023braincomputer</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Koike, Yuto and Hiroi, Yuichi and Itoh, Yuta and Rekimoto, Jun}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Brain-Computer Interface using Directional Auditory Perception}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450399845}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3582700.3583713}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3582700.3583713}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Augmented Humans International Conference 2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{342–345}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{BCI, BMI, Brain-computer Interface, Brain-machine Interface, Directional Auditory Sensation, EEG, Electroencephalogram, Electroencephalography}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Glasgow, United Kingdom}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{AHs '23}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#C32B72"> <div>Opt. Exp.</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2022-neural-distortion-480.webp 480w,/assets/img/publication_preview/2022-neural-distortion-800.webp 800w,/assets/img/publication_preview/2022-neural-distortion-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2022-neural-distortion.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2022-neural-distortion.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hiroi2022neural" class="col-sm-8"> <div class="title">Neural distortion fields for spatial calibration of wide field-of-view near-eye displays</div> <div class="author"> <em>Yuichi Hiroi</em>, Kiyosato Someya, and <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a> </div> <div class="periodical"> <em>Opt. Express</em>, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awarded</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1364/OE.472288" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.youtube.com/watch?v=Q6XjDvBP4kg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Japan’s Leading Optics Research in 2022 (Top 30), Optical Society of Japan</p> </div> <div class="abstract hidden"> <p>We propose a spatial calibration method for wide field-of-view (FoV) near-eye displays (NEDs) with complex image distortions. Image distortions in NEDs can destroy the reality of the virtual object and cause sickness. To achieve distortion-free images in NEDs, it is necessary to establish a pixel-by-pixel correspondence between the viewpoint and the displayed image. Designing compact and wide-FoV NEDs requires complex optical designs. In such designs, the displayed images are subject to gaze-contingent, non-linear geometric distortions, which explicit geometric models can be difficult to represent or computationally intensive to optimize. To solve these problems, we propose neural distortion field (NDF), a fully-connected deep neural network that implicitly represents display surfaces complexly distorted in spaces. NDF takes spatial position and gaze direction as input and outputs the display pixel coordinate and its intensity as perceived in the input gaze direction. We synthesize the distortion map from a novel viewpoint by querying points on the ray from the viewpoint and computing a weighted sum to project output display coordinates into an image. Experiments showed that NDF calibrates an augmented reality NED with 90° FoV with about 3.23 pixel (5.8 arcmin) median error using only 8 training viewpoints. Additionally, we confirmed that NDF calibrates more accurately than the non-linear polynomial fitting, especially around the center of the FoV.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hiroi2022neural</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hiroi, Yuichi and Someya, Kiyosato and Itoh, Yuta}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Opt. Express}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Image metrics; Lens design; Near eye displays; Optical aberration; Optical systems; Systems design}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{22}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{40628--40644}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Optica Publishing Group}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Neural distortion fields for spatial calibration of wide field-of-view near-eye displays}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{30}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://opg.optica.org/oe/abstract.cfm?URI=oe-30-22-40628}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1364/OE.472288}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACM VRST</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2022-NeARportation-480.webp 480w,/assets/img/publication_preview/2022-NeARportation-800.webp 800w,/assets/img/publication_preview/2022-NeARportation-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2022-NeARportation.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2022-NeARportation.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hiroi2022nearportation" class="col-sm-8"> <div class="title">NeARportation: A Remote Real-time Neural Rendering Framework</div> <div class="author"> <em>Yuichi Hiroi</em>, <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a>, and <a href="https://lab.rekimoto.org/members/rekimoto/" rel="external nofollow noopener" target="_blank">Jun Rekimoto</a> </div> <div class="periodical"> <em>In Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology</em>, Tsukuba, Japan, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3562939.3565616" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2022_VRST_NeARportation.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=mbbEZ7QcJfE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>While presenting a photorealistic appearance plays a major role in immersion in Augmented Virtuality environment, displaying that of real objects remains a challenge. Recent developments in photogrammetry have facilitated the incorporation of real objects into virtual space. However, reproducing complex appearances, such as subsurface scattering and transparency, still requires a dedicated environment for measurement and possesses a trade-off between rendering quality and frame rate. Our NeARportation framework combines server–client bidirectional communication and neural rendering to resolve these trade-offs. Neural rendering on the server receives the client’s head posture and generates a novel-view image with realistic appearance reproduction that is streamed onto the client’s display. By applying our framework to a stereoscopic display, we confirm that it can display a high-fidelity appearance on full-HD stereo videos at 35-40 frames per second (fps) according to the user’s head motion.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hiroi2022nearportation</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hiroi, Yuichi and Itoh, Yuta and Rekimoto, Jun}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{NeARportation: A Remote Real-time Neural Rendering Framework}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450398893}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3562939.3565616}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3562939.3565616}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{23}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{remote rendering, real-time rendering, neural rendering, augmented virtuality, appearance reproduction}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Tsukuba, Japan}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{VRST '22}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE ISMAR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2022-spatial-airflow-interaction-480.webp 480w,/assets/img/publication_preview/2022-spatial-airflow-interaction-800.webp 800w,/assets/img/publication_preview/2022-spatial-airflow-interaction-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2022-spatial-airflow-interaction.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2022-spatial-airflow-interaction.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhibin2022airflow" class="col-sm-8"> <div class="title">Towards Spatial Airflow Interaction: Schlieren Imaging for Augmented Reality</div> <div class="author"> Zhang Zhibin, <em>Yuichi Hiroi</em>, and <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a> </div> <div class="periodical"> <em>In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</em>, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ISMAR55827.2022.00036" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2022_ISMAR_Schlieren.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=-rIOl6YBGkU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>This work integrates Schlieren Imaging, a unique sensing modality, into Augmented Reality (AR) to explore ways to utilize invisible airflows for AR. Schlieren imaging is an imaging technique that visualizes the flow of fluids, which is normally invisible to the eyes. Theoretically, the technique can calculate the motion, pressure, temperature, and density of the airflow in our physical world. This unique, but less applied modality may expand interaction paradigms in AR and VR. We build a proof-of-concept AR system combined with Schlieren imaging that allows real airflow to affect virtual objects. The results of quantitative analyses show that our system can integrate different types of airflow with pressure values ranging from weak breathing actions to a heat gun up to 10m/s or 0.25m3/min airflow. We also showcase AR use cases including blowing out a virtual candle and a heat gun.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhibin2022airflow</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhibin, Zhang and Hiroi, Yuichi and Itoh, Yuta}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards Spatial Airflow Interaction: Schlieren Imaging for Augmented Reality}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{215-223}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Heating systems;Temperature sensors;Photography;Visualization;Sensitivity;Statistical analysis;User interfaces;Schlieren imaging;augmented reality;spatial airflow interaction}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ISMAR55827.2022.00036}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1554-7868}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#C32B72"> <div>Opt. Exp.</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2021-focal-surface-occlusion-480.webp 480w,/assets/img/publication_preview/2021-focal-surface-occlusion-800.webp 800w,/assets/img/publication_preview/2021-focal-surface-occlusion-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2021-focal-surface-occlusion.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2021-focal-surface-occlusion.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hiroi2021focalsurface" class="col-sm-8"> <div class="title">Focal surface occlusion</div> <div class="author"> <em>Yuichi Hiroi</em>, <a href="https://kamino410.github.io/" rel="external nofollow noopener" target="_blank">Takumi Kaminokado</a>, <a href="https://sites.google.com/site/thunsukeono/" rel="external nofollow noopener" target="_blank">Shunsuke Ono</a>, and <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a> </div> <div class="periodical"> <em>Opt. Express</em>, Oct 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1364/OE.440024" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>This paper proposes focal surface occlusion to provide focal cues of occlusion masks for multiple virtual objects at continuous depths in an occlusion-capable optical see-through head-mounted display. A phase-only spatial light modulator (PSLM) that acts as a dynamic free-form lens is used to conform the focal surface of an occlusion mask to the geometry of the virtual scene. To reproduce multiple and continuous focal blurs while reducing the distortion of the see-through view, an optical design based on afocal optics and edge-based optimization to exploit a property of the occlusion mask is established. The prototype with the PSLM and transmissive liquid crystal display can reproduce the focus blur of occluded objects at multiple and continuous depths with a field of view of 14.6°.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hiroi2021focalsurface</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hiroi, Yuichi and Kaminokado, Takumi and Ono, Shunsuke and Itoh, Yuta}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Opt. Express}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Head mounted displays; Microlens arrays; Near eye displays; Optical components; Optical systems; Spatial light modulators}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{22}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{36581--36597}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Optica Publishing Group}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Focal surface occlusion}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{29}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://opg.optica.org/oe/abstract.cfm?URI=oe-29-22-36581}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1364/OE.440024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Augmented Humans</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2021-circadianvisor-480.webp 480w,/assets/img/publication_preview/2021-circadianvisor-800.webp 800w,/assets/img/publication_preview/2021-circadianvisor-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2021-circadianvisor.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2021-circadianvisor.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tochimoto2021circadian" class="col-sm-8"> <div class="title">CircadianVisor: Image Presentation with an Optical See-Through Display in Consideration of Circadian Illuminance</div> <div class="author"> Takumi Tochimoto, <em>Yuichi Hiroi</em>, and <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a> </div> <div class="periodical"> <em>In Proceedings of the Augmented Humans International Conference 2021</em>, Rovaniemi, Finland, Oct 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3458709.3458938" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2021_AHs_CircadianVisor.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In modern society, the impact of nighttime artificial lighting on the human sleep/wake cycle (the circadian rhythm), has long been an important issue. In augmented reality, such health hazards should be prevented if we are to become a society that wears optical see-through head-mounted displays (OST-HMDs) on a daily basis. We present CircadianVisor, an OST display system that controls circadian performance. Our system combines an OST-HMD with a liquid crystal (LC) shutter and a spectrometer to control the circadian illuminance (CIL, biolux) of light incident on the user’s eyes. To prevent the CIL at the eyes from exceeding the threshold, correct for the displayed image based on RGB values, and adjust the transmittance of the LC visor to pass through the environment light based on spectral measurements, we build a proof-of-concept system to evaluate the feasibility of the system’s CIL control and test it with a spectrometer installed at the user’s viewpoint. The evaluation shows that the CIL at the user’s viewpoint can be kept below the threshold.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tochimoto2021circadian</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tochimoto, Takumi and Hiroi, Yuichi and Itoh, Yuta}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{CircadianVisor: Image Presentation with an Optical See-Through Display in Consideration of Circadian Illuminance}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450384285}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3458709.3458938}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3458709.3458938}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Augmented Humans International Conference 2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{66–76}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Augmented reality, Blue-light-blocking glasses, Circadian rhythm, Head-mounted displays}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Rovaniemi, Finland}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{AHs '21}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#666666"> <div>ISMAR Poster</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2021-focus-aware-retinal-480.webp 480w,/assets/img/publication_preview/2021-focus-aware-retinal-800.webp 800w,/assets/img/publication_preview/2021-focus-aware-retinal-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2021-focus-aware-retinal.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2021-focus-aware-retinal.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kaneko2021focusaware" class="col-sm-8"> <div class="title">Focus-Aware Retinal Projection-based Near-Eye Display</div> <div class="author"> Mayu Kaneko, <em>Yuichi Hiroi</em>, and <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a> </div> <div class="periodical"> <em>In 2021 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)</em>, Oct 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ISMAR-Adjunct54149.2021.00049" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2021_ISMAR_SightProjector.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=VU_jKiKZ658" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>The primary challenge in optical see-through near-eye displays lies in providing correct optical focus cues. Established approaches such as varifocal or light field display essentially sacrifice temporal or spatial resolution of the resulting 3D images. This paper explores a new direction to address the trade-off by combining a retinal projection display (RPD) with ocular wavefront sensing (OWS). Our core idea is to display a depth of field-simulated image on an RPD to produce visually consistent optical focus cues while maintaining the spatial and temporal resolution of the image. To obtain the current accommodation of the eye, we integrate OWS. We demonstrate that our proof-of-concept system successfully renders virtual contents with proper depth cues while covering the eye accommodation range from 28.5 cm (3.5 D) to infinity (0.0 D).</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">kaneko2021focusaware</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kaneko, Mayu and Hiroi, Yuichi and Itoh, Yuta}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2021 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Focus-Aware Retinal Projection-based Near-Eye Display}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{207-208}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Three-dimensional displays;Optical variables measurement;Optical imaging;Retina;Rendering (computer graphics);Adaptive optics;Light fields;Augmented reality;accommodation sensing;retinal projection;near-eye display}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ISMAR-Adjunct54149.2021.00049}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#C32B72"> <div>IEEE TVCG</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2020-stainedview-480.webp 480w,/assets/img/publication_preview/2020-stainedview-800.webp 800w,/assets/img/publication_preview/2020-stainedview-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2020-stainedview.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2020-stainedview.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kamino2020stainedview" class="col-sm-8"> <div class="title">StainedView: Variable-Intensity Light-Attenuation Display with Cascaded Spatial Color Filtering for Improved Color Fidelity</div> <div class="author"> <a href="https://kamino410.github.io/" rel="external nofollow noopener" target="_blank">Takumi Kaminokado</a>, <em>Yuichi Hiroi</em>, and <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a> </div> <div class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/TVCG.2020.3023569" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2020_ISMAR_StainedView.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We present StainedView, an optical see-through display that spatially filters the spectral distribution of light to form an image with improved color fidelity. Existing light-attenuation displays have limited color fidelity and contrast, resulting in a degraded appearance of virtual images. To use these displays to present virtual images that are more consistent with the real world, we require three things: intensity modulation of incoming light, spatial color filtering with narrower bandwidth, and appropriate light modulation for incoming light with an arbitrary spectral distribution. In StainedView, we address the three requirements by cascading two phase-only spatial light modulators (PSLMs), a digital micromirror device, and polarization optics to control both light intensity and spectrum distribution. We show that our design has a 1.8 times wider color gamut fidelity (75.8% fulfillment of sRGB color space) compared to the existing single-PSLM approach (41.4%) under a reference white light. We demonstrated the design with a proof-of-concept display system. We further introduce our optics design and pixel-selection algorithm for the given light input, evaluate the spatial color filter, and discuss the limitation of the current prototype.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kamino2020stainedview</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kaminokado, Takumi and Hiroi, Yuichi and Itoh, Yuta}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Visualization and Computer Graphics}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{StainedView: Variable-Intensity Light-Attenuation Display with Cascaded Spatial Color Filtering for Improved Color Fidelity}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{26}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3576-3586}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Image color analysis;Optical imaging;Optical polarization;Adaptive optics;Optical distortion;Optical modulation;Nonlinear optics;Light attenuation display;phase modulation;see-through display;vision augmentation;augmented reality}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TVCG.2020.3023569}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1941-0506}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Augmented Humans</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2020-dehazeglasses-480.webp 480w,/assets/img/publication_preview/2020-dehazeglasses-800.webp 800w,/assets/img/publication_preview/2020-dehazeglasses-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2020-dehazeglasses.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2020-dehazeglasses.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hiroi2020dehaze" class="col-sm-8"> <div class="title">DehazeGlasses: Optical Dehazing with an Occlusion Capable See-Through Display</div> <div class="author"> <em>Yuichi Hiroi</em>, <a href="https://kamino410.github.io/" rel="external nofollow noopener" target="_blank">Takumi Kaminokado</a>, Atsushi Mori, and <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a> </div> <div class="periodical"> <em>In Proceedings of the Augmented Humans International Conference</em>, Kaiserslautern, Germany, Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3384657.3384781" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2020_AHs_DehazeGlasses.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We present DehazeGlasses, a see-through visual haze removal system that optically dehazes the user’s field of vision. Human vision suffers from a degraded view due to aspects of the scene environment, such as haze. Such degradation may interfere with our behavior or judgement in daily tasks. We focus on hazy scenes as one common degradation source, which whitens the view due to certain atmospheric conditions. Unlike typical computer vision systems that process recorded images, we aim to realize a see-through glasses system that can optically manipulate our field of view to dehaze the perceived scene. Our system selectively modulates the intensity of the light entering the eyes via occlusion-capable optical see-through head-mounted displays (OST-HMD). We built a proof-of-concept system to evaluate the feasibility of our haze removal method by combining a digital micromirror device (DMD) and an OST-HMD, and tested it with a user-perspective viewpoint camera. A quantitative evaluation with 80 scenes from a haze removal dataset shows that our system realizes a dehazed view that is significantly closer to the ground truth scene compared to the native view under a perceptual image similarity metric. This evaluation shows that our system achieves perceptually natural haze removal while maintaining the see-through view of actual scenes.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hiroi2020dehaze</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hiroi, Yuichi and Kaminokado, Takumi and Mori, Atsushi and Itoh, Yuta}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DehazeGlasses: Optical Dehazing with an Occlusion Capable See-Through Display}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450376037}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3384657.3384781}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3384657.3384781}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Augmented Humans International Conference}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Vision Augmentation, Occlusion-Capable HMD, Head-Mounted Displays, Haze Removal, Augmented Reality}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Kaiserslautern, Germany}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{AHs '20}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#666666"> <div>ISMAR Poster</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2020-stencil-markers-480.webp 480w,/assets/img/publication_preview/2020-stencil-markers-800.webp 800w,/assets/img/publication_preview/2020-stencil-markers-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2020-stencil-markers.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2020-stencil-markers.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2020stencil" class="col-sm-8"> <div class="title">Stencil Marker: Designing Partially Transparent Markers for Stacking Augmented Reality Objects</div> <div class="author"> Xuan Zhang, Jonathan Lundgren, Yoya Mesaki, <em>Yuichi Hiroi</em>, and <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a> </div> <div class="periodical"> <em>In 2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)</em>, Nov 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ISMAR-Adjunct51615.2020.00073" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2020_ISMAR_StencilMarker.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=xerb_o7bitI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>We propose a transparent colored AR marker that allows 3D objects to be stacked in space. Conventional AR markers make it difficult to display multiple objects in the same position in space, or to manipulate the order or rotation of objects. The proposed transparent colored markers are designed to detect the order and rotation direction of each marker in the stack from the observed image, based on mathematical constraints. We describe these constraints to design markers, the implementation to detect its stacking order and rotation of each marker, and a proof-of-concept application Totem Poles. We also discuss the limitations of the current prototype and possible research directions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2020stencil</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Xuan and Lundgren, Jonathan and Mesaki, Yoya and Hiroi, Yuichi and Itoh, Yuta}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Stencil Marker: Designing Partially Transparent Markers for Stacking Augmented Reality Objects}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{255-257}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Human computer interaction;Three-dimensional displays;Image color analysis;Stacking;Prototypes;Augmented reality;Human-centered computing;Visualization;Visualization techniques;Treemaps;Human-centered computing;Visualization;Visualization design and evaluation methods}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ISMAR-Adjunct51615.2020.00073}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#666666"> <div>ISMAR Poster</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2019-ostnet-480.webp 480w,/assets/img/publication_preview/2019-ostnet-800.webp 800w,/assets/img/publication_preview/2019-ostnet-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2019-ostnet.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2019-ostnet.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="someya2019ostnet" class="col-sm-8"> <div class="title">OSTNet: Calibration Method for Optical See-Through Head-Mounted Displays via Non-Parametric Distortion Map Generation</div> <div class="author"> Kiyosato Someya, <em>Yuichi Hiroi</em>, <a href="https://riken-yamada.github.io/profile.html" rel="external nofollow noopener" target="_blank">Makoto Yamada</a>, and <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a> </div> <div class="periodical"> <em>In 2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)</em>, Oct 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ISMAR-Adjunct.2019.00-34" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2019_ISMAR_OSTNet.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We propose a spatial calibration method for Optical See-Through Head-Mounted Displays (OST-HMDs) having complex optical distortion such as wide field-of-view (FoV) designs. Viewpoint-dependent non-linear optical distortion makes existing spatial calibration methods either impossible to handle or difficult to compensate without intensive computation. To overcome this issue, we propose OSTNet, a non-parametric data-driven calibration method that creates a generative 2D distortion model for a given six-degree-of-freedom viewpoint pose.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">someya2019ostnet</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Someya, Kiyosato and Hiroi, Yuichi and Yamada, Makoto and Itoh, Yuta}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{OSTNet: Calibration Method for Optical See-Through Head-Mounted Displays via Non-Parametric Distortion Map Generation}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{259-260}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Optical distortion;Calibration;Cameras;Nonlinear distortion;Two dimensional displays;Decoding;optical see through head mounted display;calibration;Variational Autoencoder}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ISMAR-Adjunct.2019.00-34}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#C32B72"> <div>IEEE TVCG</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2018-hysar-480.webp 480w,/assets/img/publication_preview/2018-hysar-800.webp 800w,/assets/img/publication_preview/2018-hysar-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2018-hysar.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2018-hysar.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hamasaki2018hysar" class="col-sm-8"> <div class="title">HySAR: Hybrid Material Rendering by an Optical See-Through Head-Mounted Display with Spatial Augmented Reality Projection</div> <div class="author"> Takumi Hamasaki, <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a>, <em>Yuichi Hiroi</em>, <a href="https://daisukeiwai.org/" rel="external nofollow noopener" target="_blank">Daisuke Iwai</a>, and <a href="https://im-lab.net/maki-sugimoto/" rel="external nofollow noopener" target="_blank">Maki Sugimoto</a> </div> <div class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, Apr 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/TVCG.2018.2793659" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2018_IEEEVR_HySAR.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Spatial augmented reality (SAR) pursues realism in rendering materials and objects. To advance this goal, we propose a hybrid SAR (HySAR) that combines a projector with optical see-through head-mounted displays (OST-HMD). In an ordinary SAR scenario with co-located viewers, the viewers perceive the same virtual material on physical surfaces. In general, the material consists of two components: a view-independent (VI) component such as diffuse reflection, and a view-dependent (VD) component such as specular reflection. The VI component is static over viewpoints, whereas the VD should change for each viewpoint even if a projector can simulate only one viewpoint at one time. In HySAR, a projector only renders the static VI components. In addition, the OST-HMD renders the dynamic VD components according to the viewer’s current viewpoint. Unlike conventional SAR, the HySAR concept theoretically allows an unlimited number of co-located viewers to see the correct material over different viewpoints. Furthermore, the combination enhances the total dynamic range, the maximum intensity, and the resolution of perceived materials. With proof-of-concept systems, we demonstrate HySAR both qualitatively and quantitatively with real objects. First, we demonstrate HySAR by rendering synthetic material properties on a real object from different viewpoints. Our quantitative evaluation shows that our system increases the dynamic range by 2.24 times and the maximum intensity by 2.12 times compared to an ordinary SAR system. Second, we replicate the material properties of a real object by SAR and HySAR, and show that HySAR outperforms SAR in rendering VD specular components.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hamasaki2018hysar</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hamasaki, Takumi and Itoh, Yuta and Hiroi, Yuichi and Iwai, Daisuke and Sugimoto, Maki}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Visualization and Computer Graphics}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{HySAR: Hybrid Material Rendering by an Optical See-Through Head-Mounted Display with Spatial Augmented Reality Projection}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{24}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1457-1466}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Augmented realtiy;Head-mounted displays;Rendering (computer graphics);Optical reflection;Adaptive optics;Optical see-through displays;hybrid material rendering;spatial augmented reality}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TVCG.2018.2793659}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1941-0506}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Augmented Human</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2017-adaptivisor-480.webp 480w,/assets/img/publication_preview/2017-adaptivisor-800.webp 800w,/assets/img/publication_preview/2017-adaptivisor-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2017-adaptivisor.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2017-adaptivisor.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hiroi2017adaptivisor" class="col-sm-8"> <div class="title">AdaptiVisor: assisting eye adaptation via occlusive optical see-through head-mounted displays</div> <div class="author"> <em>Yuichi Hiroi</em>, <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a>, Takumi Hamasaki, and <a href="https://im-lab.net/maki-sugimoto/" rel="external nofollow noopener" target="_blank">Maki Sugimoto</a> </div> <div class="periodical"> <em>In Proceedings of the 8th Augmented Human International Conference</em>, Silicon Valley, California, USA, Apr 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Best Paper 3rd Place</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3041164.3041178" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2017_AH_AdaptiVisor.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=fi4PjGIbuu8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="award hidden d-print-inline"> <p> </p> </div> <div class="abstract hidden"> <p>Brightness adaptation is a fundamental ability in human visual system, and adjusts various levels of darkness and light. While this ability is continuously used, and it can mostly handle sudden lighting changes in the environment, the adaptation could still take several minutes. Moreover, during the adaptation, the color perception changes as well. This slow reactivity and perception change of the eyes could lead to mistakes for tasks performed in dazzling or temporally high-contrast environments such as when driving into the sun or during a welding process.We present AdaptiVisor, a vision augmentation system that assists the brightness adaptation of the eye. Our system selectively modulates the intensity of the light coming into the eyes via occlusion-capable Optical See-Through Head-Mounted Displays (OST-HMD). An integrated camera captures highlights and brightness in the environment via high-dynamic range capture, and our display system selectively dims or enhances part of field of views so that the user would not perceive rapid brightness changes. We build a proof-of-concept system to evaluate the feasibility of the adaptation assistance by combining a transmissive LCD panel and an OST-HMD, and test it with a user-perspective, view-point camera. The evaluation shows that the system decreases the overexposed area in a scene to 1/15th, and enhances the color by reducing majorly underexposed area to half. We also include a preliminary user trial and it indicates that the system also works for real eyes for the HMD part and to some extent for the LCD.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hiroi2017adaptivisor</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hiroi, Yuichi and Itoh, Yuta and Hamasaki, Takumi and Sugimoto, Maki}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AdaptiVisor: assisting eye adaptation via occlusive optical see-through head-mounted displays}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450348355}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3041164.3041178}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3041164.3041178}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 8th Augmented Human International Conference}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{AdaptiVisor, augmented reality, brightness adaptation, head-mounted displays, near-eye displays, occlusive HMD, optical see-through, vision augmentation}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Silicon Valley, California, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{AH '17}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#666666"> <div>IEEE VR Poster</div> </abbr> </div> <div id="7892251" class="col-sm-8"> <div class="title">HySAR: Hybrid material rendering by an optical see-through head-mounted display with spatial augmented reality projection</div> <div class="author"> <em>Yuichi Hiroi</em>, <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a>, Takumi Hamasaki, <a href="https://daisukeiwai.org/" rel="external nofollow noopener" target="_blank">Daisuke Iwai</a>, and <a href="https://im-lab.net/maki-sugimoto/" rel="external nofollow noopener" target="_blank">Maki Sugimoto</a> </div> <div class="periodical"> <em>In 2017 IEEE Virtual Reality (VR)</em>, Mar 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/VR.2017.7892251" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2017_IEEEVR_HySAR.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We propose a hybrid SAR concept combining a projector and Optical See-Through Head-Mounted Displays (OST-HMD). Our proposed hybrid SAR system utilizes OST-HMD as an extra rendering layer to render a view-dependent property in OST-HMDs according to the viewer’s viewpoint. Combined with view-independent components created by a static projector, the viewer can see richer material contents. Unlike conventional SAR systems, our system theoretically allows unlimited number of viewers seeing enhanced contents in the same space while keeping the existing SAR experiences. Furthermore, the system enhances the total dynamic range, the maximum intensity, and the resolution of perceived materials. With a proof-of-concept system that consists of a projector and an OST-HMD, we qualitatively demonstrate that our system successfully creates hybrid rendering on a hemisphere object from five horizontal viewpoints. Our quantitative evaluation also shows that our system increases the dynamic range by 2.1 times and the maximum intensity by 1.9 times compared to an ordinary SAR system.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">7892251</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hiroi, Yuichi and Itoh, Yuta and Hamasaki, Takumi and Iwai, Daisuke and Sugimoto, Maki}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2017 IEEE Virtual Reality (VR)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{HySAR: Hybrid material rendering by an optical see-through head-mounted display with spatial augmented reality projection}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{211-212}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Rendering (computer graphics);Dynamic range;Spatial resolution;Cameras;Optical imaging;Electronic mail;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/VR.2017.7892251}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2375-5334}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACM TEI</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2016-marcut-480.webp 480w,/assets/img/publication_preview/2016-marcut-800.webp 800w,/assets/img/publication_preview/2016-marcut-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2016-marcut.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2016-marcut.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kikuchi2016marcut" class="col-sm-8"> <div class="title">MARCut: Marker-based Laser Cutting for Personal Fabrication on Existing Objects</div> <div class="author"> Takashi Kikuchi, <em>Yuichi Hiroi</em>, Ross T. Smith, <a href="https://people.unisa.edu.au/Bruce.Thomas" rel="external nofollow noopener" target="_blank">Bruce H. Thomas</a>, and <a href="https://im-lab.net/maki-sugimoto/" rel="external nofollow noopener" target="_blank">Maki Sugimoto</a> </div> <div class="periodical"> <em>In Proceedings of the TEI ’16: Tenth International Conference on Tangible, Embedded, and Embodied Interaction</em>, Eindhoven, Netherlands, Mar 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/2839462.2856549" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Typical personal fabrication using a laser cutter allows objects to be created from raw material and the engraving of existing objects. Current methods to precisely align an object with the laser is a difficult process due to indirect manipulations. In this paper, we propose a marker-based system as a novel paradigm for direct interactive laser cutting on existing objects. Our system, MARCut, performs the laser cutting based on tangible markers that are applied directly onto the object to express the design. Two types of markers are available; hand constructed Shape Markers that represent the desired geometry, and Command Markers that indicate the operational parameters such as cut, engrave or material.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">kikuchi2016marcut</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kikuchi, Takashi and Hiroi, Yuichi and Smith, Ross T. and Thomas, Bruce H. and Sugimoto, Maki}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MARCut: Marker-based Laser Cutting for Personal Fabrication on Existing Objects}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450335829}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/2839462.2856549}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/2839462.2856549}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the TEI '16: Tenth International Conference on Tangible, Embedded, and Embodied Interaction}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{468–474}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Laser Cutting, Marker-based, Personal Fabrication}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Eindhoven, Netherlands}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{TEI '16}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#666666"> <div>SIGGRAPH E-tech</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2016-laplacian-vision-480.webp 480w,/assets/img/publication_preview/2016-laplacian-vision-800.webp 800w,/assets/img/publication_preview/2016-laplacian-vision-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2016-laplacian-vision.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2016-laplacian-vision.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="itoh2016laplacian" class="col-sm-8"> <div class="title">Laplacian vision: augmenting motion prediction via optical see-through head-mounted displays and projectors</div> <div class="author"> <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a>, <em>Yuichi Hiroi</em>, Jiu Otsuka, <a href="https://im-lab.net/maki-sugimoto/" rel="external nofollow noopener" target="_blank">Maki Sugimoto</a>, <a href="https://www.jeoresearch.com/research" rel="external nofollow noopener" target="_blank">Jason Orlosky</a>, <a href="https://carelab.info/ja/kiyoshi-kiyokawa/" rel="external nofollow noopener" target="_blank">Kiyoshi Kiyokawa</a>, and <a href="https://www.cit.tum.de/cit/startseite/" rel="external nofollow noopener" target="_blank">Gudrun Klinker</a> </div> <div class="periodical"> <em>In ACM SIGGRAPH 2016 Emerging Technologies</em>, Anaheim, California, Mar 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1145/2929464.2949028" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.youtube.com/watch?v=NNLelFnU0nw" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">itoh2016laplacian</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Itoh, Yuta and Hiroi, Yuichi and Otsuka, Jiu and Sugimoto, Maki and Orlosky, Jason and Kiyokawa, Kiyoshi and Klinker, Gudrun}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Laplacian vision: augmenting motion prediction via optical see-through head-mounted displays and projectors}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450343725}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/2929464.2949028}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/2929464.2949028}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACM SIGGRAPH 2016 Emerging Technologies}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{13}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Anaheim, California}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{SIGGRAPH '16}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#666666"> <div>ISMAR Poster</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2015-remote-welding-480.webp 480w,/assets/img/publication_preview/2015-remote-welding-800.webp 800w,/assets/img/publication_preview/2015-remote-welding-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2015-remote-welding.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2015-remote-welding.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hiroi2015remote" class="col-sm-8"> <div class="title">Remote Welding Robot Manipulation Using Multi-view Images</div> <div class="author"> <em>Yuichi Hiroi</em>, Kei Obata, Katsuhiro Suzuki, Naoto Ienaga, <a href="https://im-lab.net/maki-sugimoto/" rel="external nofollow noopener" target="_blank">Maki Sugimoto</a>, <a href="http://www.hvrl.ics.keio.ac.jp/professor-saito/" rel="external nofollow noopener" target="_blank">Hideo Saito</a>, and Tadashi Takamaru </div> <div class="periodical"> <em>In 2015 IEEE International Symposium on Mixed and Augmented Reality</em>, Sep 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ISMAR.2015.38" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>This paper proposes a remote welding robot manipulation system by using multi-view images. After an operator specifies two-dimensional path on images, the system transforms it into three-dimensional path and displays the movement of the robot by overlaying graphics with images. The accuracy of our system is sufficient to weld objects when combining with a sensor in the robot. The system allows the non-expert operator to weld objects remotely and intuitively, without the need to create a 3D model of a processed object beforehand.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hiroi2015remote</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hiroi, Yuichi and Obata, Kei and Suzuki, Katsuhiro and Ienaga, Naoto and Sugimoto, Maki and Saito, Hideo and Takamaru, Tadashi}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2015 IEEE International Symposium on Mixed and Augmented Reality}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Remote Welding Robot Manipulation Using Multi-view Images}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{128-131}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Cameras;Welding;Robot vision systems;Three-dimensional displays;Robot kinematics;Remote robot control;multi-view images;industrial robot;augmented reality}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ISMAR.2015.38}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Yuichi Hiroi. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/george-gca/multi-language-al-folio" rel="external nofollow noopener" target="_blank">multi-language-al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: March 14, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"> <div class="modal-footer" slot="footer"> <span class="help"> <svg version="1.0" class="ninja-examplekey" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 1280 1280"> <path d="M1013 376c0 73.4-.4 113.3-1.1 120.2a159.9 159.9 0 0 1-90.2 127.3c-20 9.6-36.7 14-59.2 15.5-7.1.5-121.9.9-255 1h-242l95.5-95.5 95.5-95.5-38.3-38.2-38.2-38.3-160 160c-88 88-160 160.4-160 161 0 .6 72 73 160 161l160 160 38.2-38.3 38.3-38.2-95.5-95.5-95.5-95.5h251.1c252.9 0 259.8-.1 281.4-3.6 72.1-11.8 136.9-54.1 178.5-116.4 8.6-12.9 22.6-40.5 28-55.4 4.4-12 10.7-36.1 13.1-50.6 1.6-9.6 1.8-21 2.1-132.8l.4-122.2H1013v110z"></path> </svg> to select </span> <span class="help"> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey" viewbox="0 0 24 24"> <path d="M0 0h24v24H0V0z" fill="none"></path> <path d="M20 12l-1.41-1.41L13 16.17V4h-2v12.17l-5.58-5.59L4 12l8 8 8-8z"></path> </svg> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey" viewbox="0 0 24 24"> <path d="M0 0h24v24H0V0z" fill="none"></path> <path d="M4 12l1.41 1.41L11 7.83V20h2V7.83l5.58 5.59L20 12l-8-8-8 8z"></path> </svg> to navigate </span> <span class="help"> <span class="ninja-examplekey esc">esc</span> to close </span> <span class="help"> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey backspace" viewbox="0 0 20 20" fill="currentColor"> <path fill-rule="evenodd" d="M6.707 4.879A3 3 0 018.828 4H15a3 3 0 013 3v6a3 3 0 01-3 3H8.828a3 3 0 01-2.12-.879l-4.415-4.414a1 1 0 010-1.414l4.414-4.414zm4 2.414a1 1 0 00-1.414 1.414L10.586 10l-1.293 1.293a1 1 0 101.414 1.414L12 11.414l1.293 1.293a1 1 0 001.414-1.414L13.414 10l1.293-1.293a1 1 0 00-1.414-1.414L12 8.586l-1.293-1.293z" clip-rule="evenodd"></path> </svg> move to parent </span> </div> </ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation menu",handler:()=>{window.location.href="/"}},{id:"nav-biography",title:"biography",description:"",section:"Navigation menu",handler:()=>{window.location.href="/biography/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation menu",handler:()=>{window.location.href="/publications/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/en-us/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2018/distill/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march & april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-newspaper-1-first-author-paper-and-newspaper-1-co-author-paper-have-been-accepted-for-journal-track-at-ieee-vr-2024",title:'<img class="emoji" title=":newspaper:" alt=":newspaper:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f0.png" height="20" width="20"> 1 first-author paper and <img class="emoji" title=":newspaper:" alt=":newspaper:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f0.png" height="20" width="20"> 1 co-author paper have been accepted for...',description:"",section:"News"},{id:"news-we-organize-1st-workshop-on-seamless-reality-ar-technologies-for-seamless-perception-and-cognition-between-cyber-and-physical-spaces-wsr-in-ieee-vr-2024",title:"We organize 1st Workshop on Seamless Reality: AR Technologies for Seamless Perception and...",description:"",section:"News"},{id:"news-the-co-author-paper-won-best-paper-honorable-mention-in-ieee-vr-2024-sparkles-trophy",title:'The co-author paper won Best Paper Honorable Mention in IEEE VR 2024 <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">...',description:"",section:"News"},{id:"news-clipboard-1-supervised-poster-paper-got-accepted-for-the-siggraph-2024-and-won-the-1st-student-research-competition-for-undergraduate-work-sparkles-trophy",title:'<img class="emoji" title=":clipboard:" alt=":clipboard:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4cb.png" height="20" width="20"> 1 supervised poster paper got accepted for the SIGGRAPH 2024 and won...',description:"",section:"News"},{id:"news-pencil-1-first-author-paper-for-the-conference-track-newspaper-1-co-author-paper-for-journal-track-and-clipboard-1-co-author-poster-paper-have-been-accepted-at-ieee-ismar-2024",title:'<img class="emoji" title=":pencil:" alt=":pencil:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dd.png" height="20" width="20"> 1 first-author paper for the conference track, <img class="emoji" title=":newspaper:" alt=":newspaper:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f0.png" height="20" width="20"> 1 co-author paper for...',description:"",section:"News"},{id:"news-we-will-organize-2nd-workshop-on-seamless-reality-ar-technologies-for-seamless-perception-and-cognition-between-cyber-and-physical-spaces-wsr-in-ieee-vr-2025",title:"We will organize 2nd Workshop on Seamless Reality: AR Technologies for Seamless Perception...",description:"",section:"News"},{id:"news-newspaper-a-co-author-journal-paper-has-been-published-in-ieee-access",title:'<img class="emoji" title=":newspaper:" alt=":newspaper:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f0.png" height="20" width="20"> A co-author journal paper has been published in IEEE Access.',description:"",section:"News"},{id:"news-newspaper-1-co-author-journal-paper-pencil-1-co-author-conference-paper-2-co-authour-workshop-papers-and-clipboard-1-co-author-poster-have-been-accepted-at-ieee-vr-2025",title:'<img class="emoji" title=":newspaper:" alt=":newspaper:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f0.png" height="20" width="20"> 1 co-author journal paper, <img class="emoji" title=":pencil:" alt=":pencil:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dd.png" height="20" width="20"> 1 co-author conference paper, 2 co-authour workshop...',description:"",section:"News"},{id:"news-pencil-1-co-author-paper-has-been-accepted-at-augmented-humans-2025",title:'<img class="emoji" title=":pencil:" alt=":pencil:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dd.png" height="20" width="20"> 1 co-author paper, has been accepted at Augmented Humans 2025.',description:"",section:"News"},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"socials-email",title:"Send an email",section:"Socials",handler:()=>{window.open("mailto:%79.%68%69%72%6F%69@%63%6C%75%73%74%65%72.%6D%75","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0000-0001-8567-6947","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=_ICkxzkAAAAJ","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/silencieuse","_blank")}},{id:"socials-dblp",title:"DBLP",section:"Socials",handler:()=>{window.open("https://dblp.org/pid/170/8742.html","_blank")}},{id:"socials-youtube",title:"YouTube",section:"Socials",handler:()=>{window.open("https://youtube.com/@yuichihiroi4408","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"lang-ja",title:"ja",section:"Languages",handler:()=>{window.location.href="/ja/publications/"}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>