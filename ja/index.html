<!DOCTYPE html> <html lang="ja"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Yuichi Hiroi </title> <meta name="author" content="Yuichi Hiroi"> <meta name="description" content="Personal research website of Yuichi Hiroi, Ph. D., investigating augmented reality, near-eye dispay optics and vision augmentation"> <meta name="keywords" content="augmented reality, optical see-through head-mounted displays, realistic visual appearance reproduction, vision measurement and vision augmentation"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/prof_hiroi_sq.jpg?28e415bc26f56eafe3304914f867cfb1"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yhiroi.github.io/ja/"> <script src="/assets/js/theme.js?7ff7ff8a7e8fb7efb59dde1e7b145b6f"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%79.%68%69%72%6F%69@%63%6C%75%73%74%65%72.%6D%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://orcid.org/0000-0001-8567-6947" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=_ICkxzkAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://twitter.com/silencieuse" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="https://dblp.org/pid/170/8742.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a> <a href="https://youtube.com/@yuichihiroi4408" title="YouTube" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i></a> <a href="/ja/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/ja/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/ja/biography/">biography </a> </li> <li class="nav-item "> <a class="nav-link" href="/ja/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="nav-item active"> <a class="nav-link" href="/"> EN-US</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Yuichi</span> Hiroi </h1> <p class="desc">廣井 裕一 / <i><a href="https://lab.cluster.mu/en/" rel="external nofollow noopener" target="_blank">クラスターメタバース研究所</a> シニアリサーチサイエンティスト 博士(工学) </i></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_hiroi_sq-480.webp 480w,/assets/img/prof_hiroi_sq-800.webp 800w,/assets/img/prof_hiroi_sq-1400.webp 1400w," type="image/webp" sizes="(min-width: 1200px) 351.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/prof_hiroi_sq.jpg?28e415bc26f56eafe3304914f867cfb1" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="prof_hiroi_sq.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>2017年に慶應義塾大学大学院開放環境科学専攻修士課程（<a href="https://im-lab.net/maki-sugimoto/" rel="external nofollow noopener" target="_blank">杉本</a>・<a href="https://lclab.org/people/yutasugiura" rel="external nofollow noopener" target="_blank">杉浦</a>研究室）修了後、<a href="https://www.rd.ntt/svlab/" rel="external nofollow noopener" target="_blank">NTTサービスエボリューション研究所</a>に研究員として入社。退職後、2019年に東京工業大学（現：東京科学大学）情報理工学院博士後期課程（<a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">伊藤研究室</a>）に進学し、2022年に博士（工学）を取得。同年より、東京大学大学院情報学環（<a href="https://lab.rekimoto.org/members/rekimoto/" rel="external nofollow noopener" target="_blank">暦本研究室</a>）にて日本学術振興会 特別研究員（PD）。2023年より現職。並行して、東京大学大学院情報学環 暦本研究室にて客員研究員。</p> <p>複合現実感、ヘッドマウントディスプレイ光学系、視覚計測、視覚拡張などの研究に従事。研究成果は、IEEE、OPTICAなど、VR/ARおよび光学分野におけるトップ国際論文誌、および IEEE VR, ISMARなどのトップ国際会議に継続的に掲載されている。また、国際会議IEEE VR, Augmented Humans, SIGGRAPHなど、国内外で表彰を受ける。</p> <p>2006年より<a href="http://lamer-e.tv" rel="external nofollow noopener" target="_blank">モーショングラフィクスデザイナー</a>としても活動しており、この経験をもとに、従来のモニターの枠を超え、現実と一体化した映像提示を目指すディスプレイ技術や、現実を超えて個々人の心象や価値選好に働きかける視覚体験の実現を目指して研究に取り組んでいます。</p> <p>詳細なCVは<a href="../assets/pdf/en-us/CV_Hiroi_new_1_a4.pdf">こちら</a>よりご覧いただけます。</p> </div> <h2> <a href="/ja/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%"> 2025 / 03 / 02</th> <td> <img class="emoji" title=":newspaper:" alt=":newspaper:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f0.png" height="20" width="20"> 1 co-author journal paper, <img class="emoji" title=":pencil:" alt=":pencil:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dd.png" height="20" width="20"> 1 co-author conference paper and <img class="emoji" title=":clipboard:" alt=":clipboard:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4cb.png" height="20" width="20"> 1 co-author poster have been accepted at <a href="https://ieeevr.org/2025/" rel="external nofollow noopener" target="_blank">IEEE VR 2025</a>. </td> </tr> <tr> <th scope="row" style="width: 20%"> 2025 / 01 / 16</th> <td> <img class="emoji" title=":newspaper:" alt=":newspaper:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f0.png" height="20" width="20"> A co-author <a href="https://ieeexplore.ieee.org/document/10843684" rel="external nofollow noopener" target="_blank">journal paper</a> has been published in IEEE Access. </td> </tr> <tr> <th scope="row" style="width: 20%"> 2024 / 11 / 17</th> <td> We will organize 2nd Workshop on <a href="https://sites.google.com/cluster.mu/2nd-ws-seamless-reality/" rel="external nofollow noopener" target="_blank">Seamless Reality: AR Technologies for Seamless Perception and Cognition between Cyber and Physical Spaces (WSR)</a> in <a href="https://ieeevr.org/2025/" rel="external nofollow noopener" target="_blank">IEEE VR 2025</a>. </td> </tr> <tr> <th scope="row" style="width: 20%"> 2024 / 10 / 21</th> <td> <img class="emoji" title=":pencil:" alt=":pencil:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dd.png" height="20" width="20"> 1 <strong>first-author</strong> paper for the conference track, <img class="emoji" title=":newspaper:" alt=":newspaper:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f0.png" height="20" width="20"> 1 co-author paper for journal track and <img class="emoji" title=":clipboard:" alt=":clipboard:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4cb.png" height="20" width="20"> 1 co-author poster paper have been accepted at <a href="https://ieeeismar.org/2024/" rel="external nofollow noopener" target="_blank">IEEE ISMAR 2024</a>. </td> </tr> <tr> <th scope="row" style="width: 20%"> 2024 / 08 / 01</th> <td> <img class="emoji" title=":clipboard:" alt=":clipboard:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4cb.png" height="20" width="20"> 1 supervised poster paper got accepted for the <a href="https://s2024.siggraph.org/" rel="external nofollow noopener" target="_blank">SIGGRAPH 2024</a> and won the <strong><a href="https://blog.siggraph.org/2024/08/student-innovators-triumph-celebrating-the-champions-of-the-siggraph-2024-student-research-competition.html/" rel="external nofollow noopener" target="_blank">1st Student Research Competition for Undergraduate Work</a></strong> <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> </td> </tr> </table> </div> </div> <h2> <a href="/ja/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#C32B72"> <div>IEEE TVCG</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2025-chromagazer-480.webp 480w,/assets/img/publication_preview/2025-chromagazer-800.webp 800w,/assets/img/publication_preview/2025-chromagazer-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/2025-chromagazer.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2025-chromagazer.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="tosa2025chromagazer" class="col-sm-8"> <div class="title">ChromaGazer: Unobtrusive Visual Modulation using Imperceptible Color Vibration for Visual Guidance</div> <div class="author"> Rinto Tosa, <a href="https://llhi.org/" rel="external nofollow noopener" target="_blank">Shingo Hattori</a>, <em>Yuichi Hiroi</em>, <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a>, and <a href="https://mvmlab.jp/jp/" rel="external nofollow noopener" target="_blank">Takefumi Hiraki</a> </div> <div class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2412.17274" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2025_IEEEVR_ChromaGazer.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Visual guidance (VG) plays an essential role in directing user attention in virtual reality (VR) and augmented reality (AR) environments. However, traditional approaches rely on explicit visual annotations, which often compromise visual clarity and increase user cognitive load. To address this issue, we propose an unobtrusive VG technique based on color vibration, a phenomenon in which rapidly alternating colors at frequencies above 25 Hz are perceived as a single intermediate color. Our work explores a perceptual state that exists between complete color fusion and visible flicker, where color differences remain detectable without conscious awareness of vibration. Through two experimental studies, we first identified the thresholds separating complete fusion, this intermediate perceptual state, and visible flicker by systematically varying color vibration parameters. Subsequently, we applied color vibrations with derived thresholds to natural image regions and validated their attention-guiding capabilities using eye-tracking measurements. The results demonstrate that controlled color vibration successfully directs user attention while maintaining low cognitive demand, providing an effective method for implementing unobtrusive VG in VR and AR systems.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tosa2025chromagazer</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tosa, Rinto and Hattori, Shingo and Hiroi, Yuichi and Itoh, Yuta and Hiraki, Takefumi}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Visualization and Computer Graphics}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ChromaGazer: Unobtrusive Visual Modulation using Imperceptible Color Vibration for Visual Guidance}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{to appear}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Visual Guidance;Imperceptible Color Vibration;Color Perception;Augmented Reality}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#C32B72"> <div>IEEE TVCG</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2024-stained-sweeper-480.webp 480w,/assets/img/publication_preview/2024-stained-sweeper-800.webp 800w,/assets/img/publication_preview/2024-stained-sweeper-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/2024-stained-sweeper.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2024-stained-sweeper.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="hiroi2024stainedsweeper" class="col-sm-8"> <div class="title">StainedSweeper: Compact, Variable-Intensity Light-Attenuation Display with Sweeping Tunable Retarders</div> <div class="author"> <em>Yuichi Hiroi</em>, <a href="https://mvmlab.jp/jp/" rel="external nofollow noopener" target="_blank">Takefumi Hiraki</a>, and <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a> </div> <div class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/TVCG.2024.3372058" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2024_IEEEVR_StainedSweeper.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/2024_IEEEVR_StainedSweeper_Supplementary.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://www.youtube.com/watch?v=ASugkZMet3o" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Light Attenuation Displays (LADs) are a type of Optical See-Through Head-Mounted Display (OST-HMD) that present images by attenuating incoming light with a pixel-wise polarizing color filter. Although LADs can display images in bright environments, there is a trade-off between the number of Spatial Light Modulators (SLMs) and the color gamut and contrast that can be expressed, making it difficult to achieve both high-fidelity image display and a small form factor. To address this problem, we propose StainedSweeper, a LAD that achieves both the wide color gamut and the variable intensity with a single SLM. Our system synchronously controls a pixel-wise Digital Micromirror Device (DMD) and a nonpixel polarizing color filter to pass light when each pixel is the desired color. By sweeping this control at high speed, the human eye perceives images in a time-multiplexed, integrated manner. To achieve this, we develop the OST-HMD design using a reflective Solc filter as a polarized color filter and a color reproduction algorithm based on the optimization of the time-multiplexing matrix for the selected primary color filters. Our proof-of-concept prototype showed that our single SLM design can produce subtractive images with variable contrast and a wider color gamut than conventional LADs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hiroi2024stainedsweeper</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hiroi, Yuichi and Hiraki, Takefumi and Itoh, Yuta}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Visualization and Computer Graphics}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{StainedSweeper: Compact, Variable-Intensity Light-Attenuation Display with Sweeping Tunable Retarders}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{30}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2682-2692}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Image color analysis;Optical filters;Filtering algorithms;Light sources;Optical imaging;Optical reflection;Optical polarization;Light attenuation display;see-through display;augmented reality;time-multiplexing;polarized color filter}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TVCG.2024.3372058}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1941-0506}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#C32B72"> <div>IEEE TVCG</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2024-dual-beaming-480.webp 480w,/assets/img/publication_preview/2024-dual-beaming-800.webp 800w,/assets/img/publication_preview/2024-dual-beaming-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/2024-dual-beaming.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2024-dual-beaming.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="aoki2024dualbeaming" class="col-sm-8"> <div class="title">Towards Co-Operative Beaming Displays: Dual Steering Projectors for Extended Projection Volume and Head Orientation Range</div> <div class="author"> Hiroto Aoki, Takumi Tochimoto, <em>Yuichi Hiroi</em>, and <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a> </div> <div class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Honorable Mention</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/TVCG.2024.3372118" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2024_IEEEVR_DualBeaming.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="award hidden d-print-inline"> <p> </p> </div> <div class="abstract hidden"> <p>Existing near-eye displays (NEDs) have trade-offs related to size, weight, computational resources, battery life, and body temperature. A recent paradigm, beaming display, addresses these trade-offs by separating the NED into a steering projector (SP) for image presentation and a passive headset worn by the user. However, the beaming display has issues with the projection area of a single SP and has severe limitations on the head orientation and pose that the user can move. In this study, we distribute dual steering projectors in the scene to extend the head orientation and pose of the beaming display by coordinating the dual projections on a passive headset. For cooperative control of each SP, we define a geometric model of the SPs and propose a calibration and projection control method designed for multiple projectors. We present implementations of the system along with evaluations showing that the precision and delay are 1.8 ∼ 5.7 mm and 14.46 ms, respectively, at a distance of about 1 m from the SPs. From this result, our prototype with multiple SPs can project images in the projection area (20 \textmm \times 30 \textmm) of the passive headset while extending the projectable head orientation. Furthermore, as applications of cooperative control by multiple SPs, we show the possibility of multiple users, improving dynamic range and binocular presentation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">aoki2024dualbeaming</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Aoki, Hiroto and Tochimoto, Takumi and Hiroi, Yuichi and Itoh, Yuta}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Visualization and Computer Graphics}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards Co-Operative Beaming Displays: Dual Steering Projectors for Extended Projection Volume and Head Orientation Range}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{30}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2309-2318}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Mirrors;Cameras;Headphones;Calibration;Optical imaging;High-speed optical techniques;Head;Near-eye display;Augmented reality;Projectors}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TVCG.2024.3372118}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1941-0506}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE ISMAR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2024-factored-sweeper-480.webp 480w,/assets/img/publication_preview/2024-factored-sweeper-800.webp 800w,/assets/img/publication_preview/2024-factored-sweeper-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/2024-factored-sweeper.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2024-factored-sweeper.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="hiroi2024factored" class="col-sm-8"> <div class="title">FactoredSweeper: Optical See-Through Display Integrating Light Attenuation and Addition with a Single Spatial Light Modulator</div> <div class="author"> <em>Yuichi Hiroi</em>, <a href="https://mvmlab.jp/jp/" rel="external nofollow noopener" target="_blank">Takefumi Hiraki</a>, and <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a> </div> <div class="periodical"> <em>In 2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/2024_ISMAR_FactoredSweeper.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/2024_ISMAR_FactoredSweeper_supplement.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://www.youtube.com/watch?v=z2S89fqwF5E" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Light Attenuation Displays (LADs), a subset of Optical See-Through Head-Mounted Displays (OST-HMDs), enable image display in bright environments by filtering incident light at the pixel level. Although recent methods have proposed single-DMD light attenuation, they do not consider additive color display and background compensation, limiting their applicability in real-world scenarios. We present FactoredSweeper, a single digital micromirror device (DMD) system that incorporates both light attenuation and addition. By synchronizing the DMD, color filter, and light source, our system generates an additive virtual image, light attenuation, and occlusion through time multiplexing. To produce the target image while compensating for the background, we optimize time-multiplexed binary DMD patterns and LED/color filter schedules using perceptually-driven non-negative matrix factorization. Simulations and prototypes demonstrate that our integrated attenuation-addition single-SLM system achieves superior dynamic range and perceptual image quality compared to conventional occlusion-capable OST-HMDs using grayscale occlusion masks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#C32B72"> <div>IEEE TVCG</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2023-low-latency-beaming-480.webp 480w,/assets/img/publication_preview/2023-low-latency-beaming-800.webp 800w,/assets/img/publication_preview/2023-low-latency-beaming-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/2023-low-latency-beaming.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2023-low-latency-beaming.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="hiroi2023lowlatency" class="col-sm-8"> <div class="title">Low-Latency Beaming Display: Implementation of Wearable, 133 μs Motion-to-Photon Latency Near-Eye Display</div> <div class="author"> <em>Yuichi Hiroi</em> , Akira Watanabe, <a href="http://yurimikawa.com/index.php" rel="external nofollow noopener" target="_blank">Yuri Mikawa</a>, and <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a> </div> <div class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, Nov 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/TVCG.2023.3320212" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2023_ISMAR_Lowlatency.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=pCUf4rUgQdg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://www.youtube.com/watch?v=veqkiiwLzeM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Slides</a> </div> <div class="abstract hidden"> <p>This paper presents a low-latency Beaming Display system with a 133 μ\mathrms motion-to-photon (M2P) latency, the delay from head motion to the corresponding image motion. The Beaming Display represents a recent near-eye display paradigm that involves a steerable remote projector and a passive wearable headset. This system aims to overcome typical trade-offs of Optical See-Through Head-Mounted Displays (OST-HMDs), such as weight and computational resources. However, since the Beaming Display projects a small image onto a moving, distant viewpoint, M2P latency significantly affects displacement. To reduce M2P latency, we propose a low-latency Beaming Display system that can be modularized without relying on expensive high-speed devices. In our system, a 2D position sensor, which is placed coaxially on the projector, detects the light from the IR-LED on the headset and generates a differential signal for tracking. An analog closed-loop control of the steering mirror based on this signal continuously projects images onto the headset. We have implemented a proof-of-concept prototype, evaluated the latency and the augmented reality experience through a user-perspective camera, and discussed the limitations and potential improvements of the prototype.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hiroi2023lowlatency</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hiroi, Yuichi and Watanabe, Akira and Mikawa, Yuri and Itoh, Yuta}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Visualization and Computer Graphics}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Low-Latency Beaming Display: Implementation of Wearable, 133 μs Motion-to-Photon Latency Near-Eye Display}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{29}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4761-4771}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Mirrors;Low latency communication;Headphones;Head;Cameras;Tracking;Two dimensional displays;Low-Latency Display;Beaming Display;Motion-to-Photon Latency;Lateral-effect Photodiodes}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TVCG.2023.3320212}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1941-0506}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#C32B72"> <div>Opt. Exp.</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2021-focal-surface-occlusion-480.webp 480w,/assets/img/publication_preview/2021-focal-surface-occlusion-800.webp 800w,/assets/img/publication_preview/2021-focal-surface-occlusion-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/2021-focal-surface-occlusion.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2021-focal-surface-occlusion.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="hiroi2021focalsurface" class="col-sm-8"> <div class="title">Focal surface occlusion</div> <div class="author"> <em>Yuichi Hiroi</em>, <a href="https://kamino410.github.io/" rel="external nofollow noopener" target="_blank">Takumi Kaminokado</a>, <a href="https://sites.google.com/site/thunsukeono/" rel="external nofollow noopener" target="_blank">Shunsuke Ono</a>, and <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a> </div> <div class="periodical"> <em>Opt. Express</em>, Oct 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1364/OE.440024" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>This paper proposes focal surface occlusion to provide focal cues of occlusion masks for multiple virtual objects at continuous depths in an occlusion-capable optical see-through head-mounted display. A phase-only spatial light modulator (PSLM) that acts as a dynamic free-form lens is used to conform the focal surface of an occlusion mask to the geometry of the virtual scene. To reproduce multiple and continuous focal blurs while reducing the distortion of the see-through view, an optical design based on afocal optics and edge-based optimization to exploit a property of the occlusion mask is established. The prototype with the PSLM and transmissive liquid crystal display can reproduce the focus blur of occluded objects at multiple and continuous depths with a field of view of 14.6°.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hiroi2021focalsurface</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hiroi, Yuichi and Kaminokado, Takumi and Ono, Shunsuke and Itoh, Yuta}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Opt. Express}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Head mounted displays; Microlens arrays; Near eye displays; Optical components; Optical systems; Spatial light modulators}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{22}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{36581--36597}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Optica Publishing Group}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Focal surface occlusion}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{29}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://opg.optica.org/oe/abstract.cfm?URI=oe-29-22-36581}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1364/OE.440024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Augmented Humans</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2020-dehazeglasses-480.webp 480w,/assets/img/publication_preview/2020-dehazeglasses-800.webp 800w,/assets/img/publication_preview/2020-dehazeglasses-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/2020-dehazeglasses.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2020-dehazeglasses.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="hiroi2020dehaze" class="col-sm-8"> <div class="title">DehazeGlasses: Optical Dehazing with an Occlusion Capable See-Through Display</div> <div class="author"> <em>Yuichi Hiroi</em>, <a href="https://kamino410.github.io/" rel="external nofollow noopener" target="_blank">Takumi Kaminokado</a>, Atsushi Mori, and <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a> </div> <div class="periodical"> <em>In Proceedings of the Augmented Humans International Conference</em>, Kaiserslautern, Germany, Oct 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3384657.3384781" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2020_AHs_DehazeGlasses.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We present DehazeGlasses, a see-through visual haze removal system that optically dehazes the user’s field of vision. Human vision suffers from a degraded view due to aspects of the scene environment, such as haze. Such degradation may interfere with our behavior or judgement in daily tasks. We focus on hazy scenes as one common degradation source, which whitens the view due to certain atmospheric conditions. Unlike typical computer vision systems that process recorded images, we aim to realize a see-through glasses system that can optically manipulate our field of view to dehaze the perceived scene. Our system selectively modulates the intensity of the light entering the eyes via occlusion-capable optical see-through head-mounted displays (OST-HMD). We built a proof-of-concept system to evaluate the feasibility of our haze removal method by combining a digital micromirror device (DMD) and an OST-HMD, and tested it with a user-perspective viewpoint camera. A quantitative evaluation with 80 scenes from a haze removal dataset shows that our system realizes a dehazed view that is significantly closer to the ground truth scene compared to the native view under a perceptual image similarity metric. This evaluation shows that our system achieves perceptually natural haze removal while maintaining the see-through view of actual scenes.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hiroi2020dehaze</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hiroi, Yuichi and Kaminokado, Takumi and Mori, Atsushi and Itoh, Yuta}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DehazeGlasses: Optical Dehazing with an Occlusion Capable See-Through Display}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450376037}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3384657.3384781}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3384657.3384781}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Augmented Humans International Conference}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Vision Augmentation, Occlusion-Capable HMD, Head-Mounted Displays, Haze Removal, Augmented Reality}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Kaiserslautern, Germany}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{AHs '20}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Augmented Human</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2017-adaptivisor-480.webp 480w,/assets/img/publication_preview/2017-adaptivisor-800.webp 800w,/assets/img/publication_preview/2017-adaptivisor-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/2017-adaptivisor.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2017-adaptivisor.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="hiroi2017adaptivisor" class="col-sm-8"> <div class="title">AdaptiVisor: assisting eye adaptation via occlusive optical see-through head-mounted displays</div> <div class="author"> <em>Yuichi Hiroi</em>, <a href="https://augvislab.github.io/people/yuta-itoh" rel="external nofollow noopener" target="_blank">Yuta Itoh</a>, Takumi Hamasaki, and <a href="https://im-lab.net/maki-sugimoto/" rel="external nofollow noopener" target="_blank">Maki Sugimoto</a> </div> <div class="periodical"> <em>In Proceedings of the 8th Augmented Human International Conference</em>, Silicon Valley, California, USA, Oct 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Best Paper 3rd Place</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3041164.3041178" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2017_AH_AdaptiVisor.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=fi4PjGIbuu8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="award hidden d-print-inline"> <p> </p> </div> <div class="abstract hidden"> <p>Brightness adaptation is a fundamental ability in human visual system, and adjusts various levels of darkness and light. While this ability is continuously used, and it can mostly handle sudden lighting changes in the environment, the adaptation could still take several minutes. Moreover, during the adaptation, the color perception changes as well. This slow reactivity and perception change of the eyes could lead to mistakes for tasks performed in dazzling or temporally high-contrast environments such as when driving into the sun or during a welding process.We present AdaptiVisor, a vision augmentation system that assists the brightness adaptation of the eye. Our system selectively modulates the intensity of the light coming into the eyes via occlusion-capable Optical See-Through Head-Mounted Displays (OST-HMD). An integrated camera captures highlights and brightness in the environment via high-dynamic range capture, and our display system selectively dims or enhances part of field of views so that the user would not perceive rapid brightness changes. We build a proof-of-concept system to evaluate the feasibility of the adaptation assistance by combining a transmissive LCD panel and an OST-HMD, and test it with a user-perspective, view-point camera. The evaluation shows that the system decreases the overexposed area in a scene to 1/15th, and enhances the color by reducing majorly underexposed area to half. We also include a preliminary user trial and it indicates that the system also works for real eyes for the HMD part and to some extent for the LCD.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hiroi2017adaptivisor</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hiroi, Yuichi and Itoh, Yuta and Hamasaki, Takumi and Sugimoto, Maki}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AdaptiVisor: assisting eye adaptation via occlusive optical see-through head-mounted displays}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450348355}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3041164.3041178}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3041164.3041178}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 8th Augmented Human International Conference}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{AdaptiVisor, augmented reality, brightness adaptation, head-mounted displays, near-eye displays, occlusive HMD, optical see-through, vision augmentation}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Silicon Valley, California, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{AH '17}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Yuichi Hiroi. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/george-gca/multi-language-al-folio" rel="external nofollow noopener" target="_blank">multi-language-al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: March 06, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"> <div class="modal-footer" slot="footer"> <span class="help"> <svg version="1.0" class="ninja-examplekey" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 1280 1280"> <path d="M1013 376c0 73.4-.4 113.3-1.1 120.2a159.9 159.9 0 0 1-90.2 127.3c-20 9.6-36.7 14-59.2 15.5-7.1.5-121.9.9-255 1h-242l95.5-95.5 95.5-95.5-38.3-38.2-38.2-38.3-160 160c-88 88-160 160.4-160 161 0 .6 72 73 160 161l160 160 38.2-38.3 38.3-38.2-95.5-95.5-95.5-95.5h251.1c252.9 0 259.8-.1 281.4-3.6 72.1-11.8 136.9-54.1 178.5-116.4 8.6-12.9 22.6-40.5 28-55.4 4.4-12 10.7-36.1 13.1-50.6 1.6-9.6 1.8-21 2.1-132.8l.4-122.2H1013v110z"></path> </svg> to select </span> <span class="help"> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey" viewbox="0 0 24 24"> <path d="M0 0h24v24H0V0z" fill="none"></path> <path d="M20 12l-1.41-1.41L13 16.17V4h-2v12.17l-5.58-5.59L4 12l8 8 8-8z"></path> </svg> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey" viewbox="0 0 24 24"> <path d="M0 0h24v24H0V0z" fill="none"></path> <path d="M4 12l1.41 1.41L11 7.83V20h2V7.83l5.58 5.59L20 12l-8-8-8 8z"></path> </svg> to navigate </span> <span class="help"> <span class="ninja-examplekey esc">esc</span> to close </span> <span class="help"> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey backspace" viewbox="0 0 20 20" fill="currentColor"> <path fill-rule="evenodd" d="M6.707 4.879A3 3 0 018.828 4H15a3 3 0 013 3v6a3 3 0 01-3 3H8.828a3 3 0 01-2.12-.879l-4.415-4.414a1 1 0 010-1.414l4.414-4.414zm4 2.414a1 1 0 00-1.414 1.414L10.586 10l-1.293 1.293a1 1 0 101.414 1.414L12 11.414l1.293 1.293a1 1 0 001.414-1.414L13.414 10l1.293-1.293a1 1 0 00-1.414-1.414L12 8.586l-1.293-1.293z" clip-rule="evenodd"></path> </svg> move to parent </span> </div> </ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation menu",handler:()=>{window.location.href="/ja/"}},{id:"nav-biography",title:"biography",description:"",section:"Navigation menu",handler:()=>{window.location.href="/ja/biography/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation menu",handler:()=>{window.location.href="/ja/publications/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/ja/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/ja/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/ja/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/ja/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/ja/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/ja/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/ja/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/ja/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/ja/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/ja/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/ja/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/ja/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/ja/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/ja/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/ja/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/ja/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/ja/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/ja/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/ja/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/ja/blog/2022/giscus-comments/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/ja/assets/pdf/pt-br/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/ja/blog/2021/diagrams/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/ja/blog/2020/twitter/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/ja/blog/2018/distill/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/ja/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/ja/blog/2015/math/"}},{id:"post-uma-postagem-com-c\xf3digo",title:"uma postagem com c\xf3digo",description:"um exemplo de uma postagem em um blog com c\xf3digo",section:"Posts",handler:()=>{window.location.href="/ja/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/ja/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march & april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/ja/blog/2015/formatting-and-links/"}},{id:"news-newspaper-1-first-author-paper-and-newspaper-1-co-author-paper-have-been-accepted-for-journal-track-at-ieee-vr-2024",title:'<img class="emoji" title=":newspaper:" alt=":newspaper:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f0.png" height="20" width="20"> 1 first-author paper and <img class="emoji" title=":newspaper:" alt=":newspaper:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f0.png" height="20" width="20"> 1 co-author paper have been accepted for...',description:"",section:"News"},{id:"news-we-organize-1st-workshop-on-seamless-reality-ar-technologies-for-seamless-perception-and-cognition-between-cyber-and-physical-spaces-wsr-in-ieee-vr-2024",title:"We organize 1st Workshop on Seamless Reality: AR Technologies for Seamless Perception and...",description:"",section:"News"},{id:"news-the-co-author-paper-won-best-paper-honorable-mention-in-ieee-vr-2024-sparkles-trophy",title:'The co-author paper won Best Paper Honorable Mention in IEEE VR 2024 <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">...',description:"",section:"News"},{id:"news-clipboard-1-supervised-poster-paper-got-accepted-for-the-siggraph-2024-and-won-the-1st-student-research-competition-for-undergraduate-work-sparkles-trophy",title:'<img class="emoji" title=":clipboard:" alt=":clipboard:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4cb.png" height="20" width="20"> 1 supervised poster paper got accepted for the SIGGRAPH 2024 and won...',description:"",section:"News"},{id:"news-pencil-1-first-author-paper-for-the-conference-track-newspaper-1-co-author-paper-for-journal-track-and-clipboard-1-co-author-poster-paper-have-been-accepted-at-ieee-ismar-2024",title:'<img class="emoji" title=":pencil:" alt=":pencil:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dd.png" height="20" width="20"> 1 first-author paper for the conference track, <img class="emoji" title=":newspaper:" alt=":newspaper:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f0.png" height="20" width="20"> 1 co-author paper for...',description:"",section:"News"},{id:"news-we-will-organize-2nd-workshop-on-seamless-reality-ar-technologies-for-seamless-perception-and-cognition-between-cyber-and-physical-spaces-wsr-in-ieee-vr-2025",title:"We will organize 2nd Workshop on Seamless Reality: AR Technologies for Seamless Perception...",description:"",section:"News"},{id:"news-newspaper-a-co-author-journal-paper-has-been-published-in-ieee-access",title:'<img class="emoji" title=":newspaper:" alt=":newspaper:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f0.png" height="20" width="20"> A co-author journal paper has been published in IEEE Access.',description:"",section:"News"},{id:"news-newspaper-1-co-author-journal-paper-pencil-1-co-author-conference-paper-and-clipboard-1-co-author-poster-have-been-accepted-at-ieee-vr-2025",title:'<img class="emoji" title=":newspaper:" alt=":newspaper:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f0.png" height="20" width="20"> 1 co-author journal paper, <img class="emoji" title=":pencil:" alt=":pencil:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dd.png" height="20" width="20"> 1 co-author conference paper and <img class="emoji" title=":clipboard:" alt=":clipboard:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4cb.png" height="20" width="20"> 1...',description:"",section:"News"},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/ja/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/ja/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/ja/projects/9_project/"}},{id:"projects-projeto-1",title:"projeto 1",description:"com imagem de fundo",section:"Projects",handler:()=>{window.location.href="/ja/projects/1_project/"}},{id:"projects-projeto-2",title:"projeto 2",description:"um projeto com imagem de fundo e coment\xe1rios do giscus",section:"Projects",handler:()=>{window.location.href="/ja/projects/2_project/"}},{id:"projects-projeto-3-com-um-nome-bem-longo",title:"projeto 3 com um nome bem longo",description:"um projeto que redireciona pra outro website",section:"Projects",handler:()=>{window.location.href="/ja/projects/3_project/"}},{id:"projects-projeto-4",title:"projeto 4",description:"outro sem imagem",section:"Projects",handler:()=>{window.location.href="/ja/projects/4_project/"}},{id:"projects-projeto-5",title:"projeto 5",description:"um projeto com imagem de fundo",section:"Projects",handler:()=>{window.location.href="/ja/projects/5_project/"}},{id:"projects-projeto-6",title:"projeto 6",description:"um projeto sem imagem",section:"Projects",handler:()=>{window.location.href="/ja/projects/6_project/"}},{id:"socials-email",title:"Send an email",section:"Socials",handler:()=>{window.open("mailto:%79.%68%69%72%6F%69@%63%6C%75%73%74%65%72.%6D%75","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0000-0001-8567-6947","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=_ICkxzkAAAAJ","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/silencieuse","_blank")}},{id:"socials-dblp",title:"DBLP",section:"Socials",handler:()=>{window.open("https://dblp.org/pid/170/8742.html","_blank")}},{id:"socials-youtube",title:"YouTube",section:"Socials",handler:()=>{window.open("https://youtube.com/@yuichihiroi4408","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"lang-en-us",title:"en-us",section:"Languages",handler:()=>{window.location.href="/"}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>